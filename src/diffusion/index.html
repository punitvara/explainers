<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Diffusion Models from First Principles</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Newsreader:ital,opsz,wght@0,6..72,300..800;1,6..72,300..800&family=Figtree:wght@300..900&display=swap" rel="stylesheet">
  <style>
    /* ===== RESET & BASE ===== */
    *, *::before, *::after {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    html {
      font-size: 21px;
      scroll-behavior: smooth;
      -webkit-font-smoothing: antialiased;
      -moz-osx-font-smoothing: grayscale;
    }

    body {
      font-family: 'Newsreader', Georgia, serif;
      font-size: 1rem;
      line-height: 1.55;
      color: #2d2d2d;
      background-color: #faf9f7;
    }

    /* ===== LAYOUT ===== */
    .layout-width {
      max-width: 820px;
      margin: 0 auto;
      padding: 0 2rem;
    }

    .content-width {
      max-width: 680px;
      margin: 0 auto;
    }

    /* ===== HERO ===== */
    .hero {
      position: relative;
      width: calc(100% - 2rem);
      max-width: 820px;
      margin: 1rem auto 0;
      border-radius: 12px;
      overflow: hidden;
      min-height: 280px;
      display: flex;
      align-items: flex-end;
      background: linear-gradient(135deg, #6366f1 0%, #8b5cf6 30%, #a855f7 50%, #ec4899 75%, #6366f1 100%);
      background-size: 300% 300%;
      animation: heroGradient 12s ease-in-out infinite;
    }

    @keyframes heroGradient {
      0% { background-position: 0% 50%; }
      50% { background-position: 100% 50%; }
      100% { background-position: 0% 50%; }
    }

    .hero__particles {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      overflow: hidden;
    }

    .hero__text {
      position: relative;
      z-index: 2;
      padding: 2.5rem 2rem 2rem;
      width: 100%;
    }

    .hero__title {
      font-family: 'Newsreader', Georgia, serif;
      font-size: 2.8rem;
      font-weight: 700;
      color: #fff;
      line-height: 1.15;
      margin-bottom: 0.5rem;
      text-shadow: 0 2px 12px rgba(0,0,0,0.15);
    }

    .hero__subtitle {
      font-family: 'Newsreader', Georgia, serif;
      font-size: 1.1rem;
      font-weight: 400;
      color: rgba(255,255,255,0.9);
      line-height: 1.4;
      max-width: 540px;
    }

    /* ===== NAV ===== */
    .site-nav {
      display: flex;
      justify-content: space-between;
      align-items: center;
      padding: 1rem 2rem;
      max-width: 820px;
      margin: 0 auto;
    }

    .site-nav__logo {
      font-family: 'Figtree', system-ui, sans-serif;
      font-weight: 700;
      font-size: 0.85rem;
      color: #2d2d2d;
      text-decoration: none;
      letter-spacing: -0.02em;
    }

    .site-nav__logo span {
      color: #6366f1;
    }

    /* ===== TABLE OF CONTENTS ===== */
    .toc-wrapper {
      max-width: 680px;
      margin: 2.5rem auto 3rem;
      padding: 0 2rem;
    }

    .toc h4 {
      font-family: 'Figtree', system-ui, sans-serif;
      font-size: 0.65rem;
      font-weight: 700;
      text-transform: uppercase;
      letter-spacing: 0.12em;
      color: #666;
      margin-bottom: 0.8rem;
    }

    .toc ol {
      list-style: none;
      counter-reset: toc-counter;
    }

    .toc ol li {
      counter-increment: toc-counter;
      margin-bottom: 0.35rem;
      font-family: 'Figtree', system-ui, sans-serif;
      font-size: 0.76rem;
      line-height: 1.4;
    }

    .toc ol li::before {
      content: counter(toc-counter, upper-roman) ". ";
      display: inline-block;
      width: 2.2em;
      font-weight: 600;
      color: #999;
    }

    .toc ol li a {
      color: #2d2d2d;
      text-decoration: none;
    }

    .toc ol li a:hover {
      color: #6366f1;
    }

    .toc ol li.active a {
      color: #6366f1;
      font-weight: 600;
    }

    /* ===== POST BODY ===== */
    .post-body {
      max-width: 680px;
      margin: 0 auto;
      padding: 0 2rem;
    }

    .post-body p {
      margin-bottom: 1.1rem;
    }

    .post-body p + p {
      margin-top: 0;
    }

    /* ===== SECTION HEADINGS ===== */
    .section-heading {
      display: flex;
      align-items: baseline;
      gap: 1rem;
      margin: 3.5rem 0 0;
      padding-bottom: 0.6rem;
      border-bottom: 1px solid #ddd;
    }

    .section-heading__number {
      font-family: 'Figtree', system-ui, sans-serif;
      font-size: 1.4rem;
      font-weight: 300;
      color: #bbb;
      min-width: 2.5rem;
      text-align: right;
      line-height: 1;
      position: relative;
      top: -0.1em;
    }

    .section-heading__number::after {
      content: ".";
    }

    .section-heading h2 {
      font-family: 'Newsreader', Georgia, serif;
      font-size: 1.9rem;
      font-weight: 600;
      color: #2d2d2d;
      line-height: 1.2;
    }

    .section-heading + .section-content {
      margin-top: 1.8rem;
    }

    .section-content {
      margin-bottom: 2rem;
    }

    /* ===== SUBHEADINGS ===== */
    .post-body h3 {
      font-family: 'Newsreader', Georgia, serif;
      font-size: 1.35rem;
      font-weight: 600;
      color: #2d2d2d;
      margin: 2.5rem 0 1rem;
      line-height: 1.3;
    }

    /* ===== LINKS ===== */
    .post-body a {
      color: #6366f1;
      text-decoration: underline;
      text-decoration-color: rgba(99, 102, 241, 0.3);
      text-underline-offset: 0.15em;
      transition: text-decoration-color 0.2s;
    }

    .post-body a:hover {
      text-decoration-color: #6366f1;
    }

    /* ===== EMPHASIS ===== */
    .post-body em {
      font-style: italic;
    }

    .post-body strong {
      font-weight: 600;
    }

    /* ===== SIDENOTES ===== */
    .sidenote {
      background: #f5f5f5;
      border-left: 3px solid #d4d4d4;
      border-radius: 0 8px 8px 0;
      padding: 1rem 1.2rem;
      margin: 1.2rem 0;
      font-style: italic;
      font-size: 0.9rem;
      line-height: 1.5;
      color: #555;
    }

    .sidenote strong {
      font-style: normal;
      color: #2d2d2d;
    }

    /* ===== FIGURES ===== */
    .post__figure {
      margin: 2rem 0;
    }

    .post__figure img,
    .post__figure svg {
      max-width: 100%;
      height: auto;
      display: block;
      margin: 0 auto;
    }

    .post__figcaption {
      font-style: italic;
      font-size: 0.8rem;
      color: #666;
      text-align: center;
      margin-top: 0.6rem;
      line-height: 1.4;
    }

    /* ===== WIDGETS (interactive elements) ===== */
    .widget {
      background: #f5f5f5;
      border-radius: 12px;
      padding: 1.5rem;
      margin: 2rem 0;
      border: 1px solid #e8e8e8;
    }

    .widget__title {
      font-family: 'Figtree', system-ui, sans-serif;
      font-size: 0.65rem;
      font-weight: 700;
      text-transform: uppercase;
      letter-spacing: 0.12em;
      color: #888;
      margin-bottom: 1rem;
    }

    /* ===== SLIDERS ===== */
    input[type="range"] {
      -webkit-appearance: none;
      appearance: none;
      width: 100%;
      height: 6px;
      border-radius: 3px;
      background: #ddd;
      outline: none;
      cursor: pointer;
    }

    input[type="range"]::-webkit-slider-thumb {
      -webkit-appearance: none;
      appearance: none;
      width: 18px;
      height: 18px;
      border-radius: 50%;
      background: #6366f1;
      cursor: pointer;
      border: 2px solid #fff;
      box-shadow: 0 1px 4px rgba(0,0,0,0.2);
    }

    input[type="range"]::-moz-range-thumb {
      width: 18px;
      height: 18px;
      border-radius: 50%;
      background: #6366f1;
      cursor: pointer;
      border: 2px solid #fff;
      box-shadow: 0 1px 4px rgba(0,0,0,0.2);
    }

    /* ===== BUTTONS ===== */
    .btn {
      font-family: 'Figtree', system-ui, sans-serif;
      font-size: 0.7rem;
      font-weight: 600;
      padding: 0.4rem 1rem;
      border: none;
      border-radius: 6px;
      cursor: pointer;
      transition: all 0.15s;
    }

    .btn--primary {
      background: #6366f1;
      color: #fff;
    }

    .btn--primary:hover {
      background: #4f46e5;
    }

    .btn--secondary {
      background: #e8e8e8;
      color: #555;
    }

    .btn--secondary:hover {
      background: #ddd;
    }

    /* ===== MATH ===== */
    .math {
      font-family: 'Newsreader', Georgia, serif;
      font-style: italic;
    }

    .math-block {
      display: block;
      text-align: center;
      margin: 1.5rem 0;
      font-size: 1.05rem;
      overflow-x: auto;
      padding: 1rem;
      background: #faf9f7;
      border-radius: 8px;
    }

    .math sub {
      font-size: 0.7em;
    }

    .math sup {
      font-size: 0.7em;
    }

    /* ===== TABLES ===== */
    .post-body table {
      width: 100%;
      border-collapse: collapse;
      margin: 1.5rem 0;
      font-size: 0.85rem;
    }

    .post-body table th {
      font-family: 'Figtree', system-ui, sans-serif;
      font-weight: 700;
      font-size: 0.7rem;
      text-transform: uppercase;
      letter-spacing: 0.06em;
      padding: 0.6rem 0.8rem;
      text-align: left;
      border-bottom: 2px solid #ddd;
      color: #666;
    }

    .post-body table td {
      padding: 0.6rem 0.8rem;
      border-bottom: 1px solid #eee;
      vertical-align: top;
    }

    /* ===== FOOTER ===== */
    .site-footer {
      background: linear-gradient(135deg, #312e81, #4338ca, #6366f1);
      color: #fff;
      padding: 3rem 2rem;
      margin-top: 4rem;
      border-radius: 12px 12px 0 0;
      max-width: 820px;
      margin-left: auto;
      margin-right: auto;
    }

    .site-footer__tagline {
      font-family: 'Figtree', system-ui, sans-serif;
      font-size: 0.75rem;
      color: rgba(255,255,255,0.7);
      margin-top: 1rem;
    }

    .site-footer__bottom {
      font-family: 'Figtree', system-ui, sans-serif;
      font-size: 0.6rem;
      text-align: center;
      color: rgba(255,255,255,0.5);
      letter-spacing: 0.1em;
      text-transform: uppercase;
      margin-top: 2rem;
      padding-top: 1.5rem;
      border-top: 1px solid rgba(255,255,255,0.15);
    }

    /* ===== RESPONSIVE ===== */
    @media (max-width: 640px) {
      html { font-size: 18px; }
      .hero__title { font-size: 2rem; }
      .section-heading h2 { font-size: 1.5rem; }
      .section-heading__number { font-size: 1.1rem; min-width: 2rem; }
      .widget { padding: 1rem; }
    }
  </style>
</head>
<body>

  <!-- NAV -->
  <nav class="site-nav">
    <a href="#" class="site-nav__logo">diffusion<span>.explained</span></a>
  </nav>

  <!-- HERO -->
  <header class="hero">
    <div class="hero__particles" id="heroParticles"></div>
    <div class="hero__text">
      <h1 class="hero__title">Diffusion Models from First Principles</h1>
      <h2 class="hero__subtitle">How adding noise teaches machines to create. From random static to stunning images.</h2>
    </div>
  </header>

  <!-- TABLE OF CONTENTS -->
  <div class="toc-wrapper">
    <div class="toc">
      <h4>Contents</h4>
      <ol>
        <li class="active"><a href="#introduction">Introduction</a></li>
        <li><a href="#core-idea">The Core Idea &mdash; Destruction as Creation</a></li>
        <li><a href="#forward-process">The Forward Process</a></li>
        <li><a href="#beautiful-shortcut">A Beautiful Shortcut</a></li>
        <li><a href="#reverse-process">The Reverse Process</a></li>
        <li><a href="#what-network-learns">What Does the Network Learn?</a></li>
        <li><a href="#training-loop">The Training Loop</a></li>
        <li><a href="#sampling">Sampling &mdash; Birth of an Image</a></li>
        <li><a href="#score-function">The Score Function Perspective</a></li>
        <li><a href="#variance-schedules">Variance Schedules &amp; Noise Levels</a></li>
        <li><a href="#speeding-up">Speeding Things Up</a></li>
        <li><a href="#latent-space">From Pixels to Latent Space</a></li>
        <li><a href="#further-resources">Further Resources</a></li>
      </ol>
    </div>
  </div>

  <!-- POST BODY -->
  <article class="post-body">

    <!-- Section I: Introduction -->
    <div id="introduction"></div>
    <div class="section-content">
      <p>Every image you've seen from Midjourney, DALL-E, or Stable Diffusion was born from pure noise. Literal static &mdash; the kind you'd see on an old untuned TV. A neural network looked at that static and, step by painstaking step, sculpted it into a photograph, a painting, a dream.</p>

      <p>The standard internet explanation of how this works is "diffusion models." And that's not wrong, but it's also not very useful. Simply knowing the name of something is very different from understanding it.</p>

      <p>So what <em>does</em> constitute understanding? My answer: having a model that allows you to make predictions. If you can reliably predict <em>how</em> and <em>why</em> each step of the process works, then you probably understand it.</p>

      <p>In this article, we'll build up diffusion models from scratch &mdash; starting from pure intuition, adding math only when it earns its keep, and building interactive demos along the way so you can <em>see</em> and <em>feel</em> every concept. By the time you're done, you won't just know what diffusion models are. You'll be able to derive them on a napkin.</p>

      <p>Here's the core insight, in one sentence: <strong>if you learn to reverse each tiny step of a destruction process, you can create from scratch.</strong></p>

      <p>Let's unpack that.</p>
    </div>

    <!-- Section II: The Core Idea -->
    <div id="core-idea"></div>
    <div class="section-heading">
      <span class="section-heading__number">II</span>
      <h2>The Core Idea &mdash; Destruction as Creation</h2>
    </div>
    <div class="section-content">
      <p>Imagine you film yourself scrambling an egg. You crack it into a pan, poke the yolk, and stir until it's a uniform yellow mush. Easy. Anyone can do it. The process is irreversible &mdash; you can never un-scramble the egg.</p>

      <p>But what if you had the film? What if you could study that film, frame by frame, and learn <em>exactly</em> what changed between each pair of consecutive frames?</p>

      <p>Each individual change is tiny &mdash; a few molecules shifting here, a bit of yolk mixing there. And tiny changes <em>are</em> learnable. If you could train a model to predict "given frame 57, what did frame 56 look like?", and you could do that for every pair of frames... you could run the film backward. You could un-scramble the egg.</p>

      <p>That's diffusion models in a nutshell.</p>

      <p>Replace "egg" with "image" and "scrambling" with "adding Gaussian noise," and you have the entire framework:</p>

      <p><strong>Forward process</strong> (the scrambling): Start with a clean image. Gradually add random noise, step by step, until it's pure static. This is trivial &mdash; no learning required.</p>

      <p><strong>Reverse process</strong> (the un-scrambling): Train a neural network to undo each tiny noise step. Then, starting from pure static, apply the learned reverse steps one by one to conjure an image from nothing.</p>

      <p>The trick is that each step only removes a <em>tiny</em> bit of noise. The network doesn't need to imagine an entire image in one shot &mdash; it just needs to make a small, local improvement. That's a much easier problem.</p>

      <p>Let's see this in action. Below is a simple 8&times;8 pixel image. Drag the slider to add noise, step by step, and watch it dissolve into static.</p>

      <!-- Interactive: The Destruction Film -->
      <div class="widget" id="destruction-widget">
        <div class="widget__title">The Destruction Film</div>
        <div style="display: flex; gap: 1.5rem; align-items: flex-start; flex-wrap: wrap;">
          <div style="flex: 1; min-width: 200px;">
            <canvas id="destruction-canvas" width="200" height="200" style="border-radius: 8px; image-rendering: pixelated; width: 200px; height: 200px; border: 1px solid #ddd;"></canvas>
          </div>
          <div style="flex: 1; min-width: 200px;">
            <div style="margin-bottom: 0.8rem;">
              <label class="widget__title" style="margin-bottom: 0.3rem; display: block;">Timestep <span class="math" id="destruction-t-label">t = 0</span></label>
              <input type="range" id="destruction-slider" min="0" max="100" value="0" style="width: 100%;">
              <div style="display: flex; justify-content: space-between; font-family: 'Figtree', sans-serif; font-size: 0.55rem; color: #999; margin-top: 0.2rem;">
                <span>Clean image</span>
                <span>Pure noise</span>
              </div>
            </div>
            <div style="margin-top: 1rem;">
              <div class="widget__title">Signal remaining</div>
              <div style="background: #ddd; border-radius: 4px; height: 12px; overflow: hidden;">
                <div id="destruction-signal-bar" style="background: linear-gradient(90deg, #6366f1, #a855f7); height: 100%; width: 100%; border-radius: 4px; transition: width 0.15s;"></div>
              </div>
            </div>
            <div style="margin-top: 0.8rem;">
              <div class="widget__title">Noise level</div>
              <div style="background: #ddd; border-radius: 4px; height: 12px; overflow: hidden;">
                <div id="destruction-noise-bar" style="background: linear-gradient(90deg, #f59e0b, #ef4444); height: 100%; width: 0%; border-radius: 4px; transition: width 0.15s;"></div>
              </div>
            </div>
          </div>
        </div>
        <p class="post__figcaption" style="margin-top: 1rem;">Drag the slider to watch a smiley face dissolve into noise. Each step adds a tiny bit of Gaussian noise. After enough steps, the original image is completely unrecoverable.</p>
      </div>

      <p>Notice something important: at the beginning, you can clearly see the smiley face even with some noise. In the middle, you can <em>kind of</em> tell something is there. By the end, it's indistinguishable from pure random static. The information has been destroyed.</p>

      <p>But here's the key: between any two adjacent timesteps, the change is small. And small changes are predictable. That's the opening we need.</p>
    </div>

    <!-- Section III: The Forward Process -->
    <div id="forward-process"></div>
    <div class="section-heading">
      <span class="section-heading__number">III</span>
      <h2>The Forward Process</h2>
    </div>
    <div class="section-content">
      <p>Let's get precise about what "adding noise step by step" means.</p>

      <p>We start with a clean image <span class="math">x<sub>0</sub></span>. At each timestep <span class="math">t</span>, we add a small amount of Gaussian noise to get a slightly noisier version <span class="math">x<sub>t</sub></span>. The amount of noise at each step is controlled by a parameter <span class="math">&beta;<sub>t</sub></span> (beta), called the <em>noise schedule</em>.</p>

      <p>The math for a single step is:</p>

      <div class="math-block">
        <span class="math">q(x<sub>t</sub> | x<sub>t-1</sub>) = N(x<sub>t</sub>; &radic;(1 - &beta;<sub>t</sub>) &middot; x<sub>t-1</sub>, &beta;<sub>t</sub> &middot; I)</span>
      </div>

      <p>In plain English: to get <span class="math">x<sub>t</sub></span> from <span class="math">x<sub>t-1</sub></span>, you slightly shrink the image (multiply by <span class="math">&radic;(1 - &beta;<sub>t</sub>)</span>, which is just under 1) and add a little Gaussian noise (with variance <span class="math">&beta;<sub>t</sub></span>).</p>

      <div class="sidenote">
        <strong>Why shrink the image?</strong> If we just added noise without shrinking, the overall magnitude of the image would grow without bound. The shrinking factor ensures the variance stays controlled. Together, the shrink + noise ensure that after enough steps, the result is a standard Gaussian: <span class="math">N(0, I)</span>. This is important &mdash; it means the endpoint of the forward process is always the same, regardless of what image you started with.
      </div>

      <p><span class="math">&beta;<sub>t</sub></span> is typically small &mdash; something like 0.0001 to 0.02 &mdash; and increases gradually over the course of the process. Early steps barely touch the image. Later steps add more noise. A typical diffusion model uses <span class="math">T = 1000</span> total steps.</p>

      <p>Here's a more detailed view. Pick a shape and watch how the pixel distribution changes as noise is added:</p>

      <!-- Interactive: Noise Kitchen -->
      <div class="widget" id="noise-kitchen-widget">
        <div class="widget__title">Noise Kitchen</div>
        <div style="display: flex; gap: 0.8rem; margin-bottom: 1rem;">
          <button class="btn btn--primary nk-shape-btn" data-shape="smiley">Smiley</button>
          <button class="btn btn--secondary nk-shape-btn" data-shape="star">Star</button>
          <button class="btn btn--secondary nk-shape-btn" data-shape="heart">Heart</button>
        </div>
        <div style="display: flex; gap: 1.5rem; align-items: flex-start; flex-wrap: wrap;">
          <div style="flex: 0 0 auto;">
            <canvas id="nk-canvas" width="200" height="200" style="border-radius: 8px; image-rendering: pixelated; width: 200px; height: 200px; border: 1px solid #ddd;"></canvas>
          </div>
          <div style="flex: 1; min-width: 200px;">
            <canvas id="nk-histogram" width="300" height="140" style="width: 100%; height: 140px; border-radius: 8px; border: 1px solid #ddd;"></canvas>
            <div style="font-family: 'Figtree', sans-serif; font-size: 0.55rem; color: #999; text-align: center; margin-top: 0.3rem;">Pixel value distribution (converges to Gaussian)</div>
          </div>
        </div>
        <div style="margin-top: 1rem;">
          <label class="widget__title" style="display: block; margin-bottom: 0.3rem;">Timestep <span class="math" id="nk-t-label">t = 0 / 1000</span></label>
          <input type="range" id="nk-slider" min="0" max="1000" value="0" style="width: 100%;">
          <div style="display: flex; justify-content: space-between; font-family: 'Figtree', sans-serif; font-size: 0.55rem; color: #999; margin-top: 0.2rem;">
            <span>t = 0 (clean)</span>
            <span>t = 1000 (pure noise)</span>
          </div>
        </div>
        <p class="post__figcaption" style="margin-top: 0.8rem;">Pick a shape, then drag the slider. The histogram on the right shows how the pixel values spread out toward a Gaussian bell curve as noise increases.</p>
      </div>

      <p>The histogram is the real story here. At <span class="math">t = 0</span>, the pixel values cluster around a few specific values (the colors of the shape). As <span class="math">t</span> increases, they spread out. By <span class="math">t = 1000</span>, they form a near-perfect bell curve &mdash; a Gaussian distribution centered at zero. The original image has been completely forgotten.</p>

      <p>This convergence to a Gaussian is not a coincidence. It's a consequence of the <em>Central Limit Theorem</em>: adding many small independent random variables always produces a Gaussian. It doesn't matter what image you started with &mdash; the end state is always the same.</p>
    </div>

    <!-- Section IV: A Beautiful Shortcut -->
    <div id="beautiful-shortcut"></div>
    <div class="section-heading">
      <span class="section-heading__number">IV</span>
      <h2>A Beautiful Shortcut</h2>
    </div>
    <div class="section-content">
      <p>Running 1000 noise steps one by one would be painfully slow during training. Fortunately, there's a beautiful mathematical shortcut.</p>

      <p>Define <span class="math">&alpha;<sub>t</sub> = 1 - &beta;<sub>t</sub></span> and <span class="math">&alpha;&#772;<sub>t</sub> = &alpha;<sub>1</sub> &middot; &alpha;<sub>2</sub> &middot; ... &middot; &alpha;<sub>t</sub></span> (the cumulative product). Then you can jump directly from the clean image <span class="math">x<sub>0</sub></span> to any timestep <span class="math">t</span> in a single shot:</p>

      <div class="math-block">
        <span class="math">x<sub>t</sub> = &radic;&alpha;&#772;<sub>t</sub> &middot; x<sub>0</sub> + &radic;(1 - &alpha;&#772;<sub>t</sub>) &middot; &epsilon;</span>
        &nbsp;&nbsp;&nbsp; where <span class="math">&epsilon; ~ N(0, I)</span>
      </div>

      <p>This is the <em>reparameterization trick</em>, and it's crucial. Instead of noising step-by-step, you can sample <em>one</em> noise vector <span class="math">&epsilon;</span> and mix it with the clean image. The mixing ratio is controlled by <span class="math">&alpha;&#772;<sub>t</sub></span>:</p>

      <ul style="margin: 1rem 0; padding-left: 1.5rem;">
        <li style="margin-bottom: 0.5rem;">When <span class="math">t</span> is small, <span class="math">&alpha;&#772;<sub>t</sub> &asymp; 1</span> &mdash; mostly signal, little noise</li>
        <li style="margin-bottom: 0.5rem;">When <span class="math">t</span> is large, <span class="math">&alpha;&#772;<sub>t</sub> &asymp; 0</span> &mdash; mostly noise, little signal</li>
      </ul>

      <div class="sidenote">
        <strong>Signal-to-Noise Ratio (SNR)</strong>: The quantity <span class="math">&alpha;&#772;<sub>t</sub> / (1 - &alpha;&#772;<sub>t</sub>)</span> is the signal-to-noise ratio at timestep <span class="math">t</span>. It starts high (clean image) and decays to near-zero (pure noise). A well-designed noise schedule ensures a smooth decay of SNR across timesteps &mdash; not too fast, not too slow. This is why the cosine schedule often works better than a linear one: it provides a smoother SNR curve.
      </div>

      <p>Why does this matter? Because during training, we need to generate noisy versions of images at random timesteps, thousands of times. The shortcut lets us do this in one operation instead of hundreds.</p>

      <!-- Interactive: Shortcut Calculator -->
      <div class="widget" id="shortcut-widget">
        <div class="widget__title">The Shortcut &mdash; Step-by-Step vs Direct Jump</div>
        <div style="display: flex; gap: 1.5rem; flex-wrap: wrap;">
          <div style="flex: 1; min-width: 180px; text-align: center;">
            <div class="widget__title">Step-by-step</div>
            <canvas id="sc-stepwise" width="140" height="140" style="image-rendering: pixelated; width: 140px; height: 140px; border-radius: 8px; border: 1px solid #ddd;"></canvas>
            <div style="font-family: 'Figtree', sans-serif; font-size: 0.55rem; color: #999; margin-top: 0.3rem;"><span id="sc-steps-count">0</span> sequential steps</div>
          </div>
          <div style="flex: 0 0 auto; display: flex; align-items: center; font-size: 1.5rem; color: #999;">=</div>
          <div style="flex: 1; min-width: 180px; text-align: center;">
            <div class="widget__title">Direct jump</div>
            <canvas id="sc-direct" width="140" height="140" style="image-rendering: pixelated; width: 140px; height: 140px; border-radius: 8px; border: 1px solid #ddd;"></canvas>
            <div style="font-family: 'Figtree', sans-serif; font-size: 0.55rem; color: #999; margin-top: 0.3rem;">1 operation</div>
          </div>
        </div>
        <div style="margin-top: 1rem;">
          <label class="widget__title" style="display: block; margin-bottom: 0.3rem;">Jump to timestep <span class="math" id="sc-t-label">t = 0</span></label>
          <input type="range" id="sc-slider" min="0" max="100" value="0" style="width: 100%;">
        </div>
        <p class="post__figcaption" style="margin-top: 0.8rem;">Both paths produce the same result. The shortcut formula lets us skip all intermediate steps during training.</p>
      </div>

      <p>Try it above: both the step-by-step path and the direct jump arrive at the same noisy image. This is not an approximation &mdash; they are mathematically <em>identical</em> (given the same noise vector).</p>
    </div>

    <!-- Section V: The Reverse Process -->
    <div id="reverse-process"></div>
    <div class="section-heading">
      <span class="section-heading__number">V</span>
      <h2>The Reverse Process</h2>
    </div>
    <div class="section-content">
      <p>Now for the million-dollar question: can we run the film backward?</p>

      <p>Mathematically, we want <span class="math">p(x<sub>t-1</sub> | x<sub>t</sub>)</span> &mdash; given a noisy image at step <span class="math">t</span>, what did it look like one step earlier? If we had this, we could start from pure noise <span class="math">x<sub>T</sub></span> and walk backward to a clean image <span class="math">x<sub>0</sub></span>.</p>

      <p>The catch: computing <span class="math">p(x<sub>t-1</sub> | x<sub>t</sub>)</span> exactly requires knowing <span class="math">p(x<sub>t</sub>)</span> &mdash; the probability distribution over all possible images at noise level <span class="math">t</span>. That's intractable. It would mean knowing every possible image that could exist.</p>

      <p>The solution: <em>approximate it with a neural network</em>.</p>

      <p>We train a network <span class="math">p<sub>&theta;</sub>(x<sub>t-1</sub> | x<sub>t</sub>)</span> that takes in a noisy image and predicts what a slightly <em>less</em> noisy version looks like. The key insight is that this reverse step is also Gaussian (when the forward steps are small enough), so the network only needs to predict two things: a <strong>mean</strong> and a <strong>variance</strong>.</p>

      <div class="sidenote">
        <strong>Why is the reverse also Gaussian?</strong> When <span class="math">&beta;<sub>t</sub></span> is small, each forward step makes a tiny perturbation. For small perturbations, the reverse of a Gaussian process is also approximately Gaussian. This is a result from stochastic differential equations. In practice, the variance is usually fixed to <span class="math">&beta;<sub>t</sub></span> or <span class="math">&beta;&#771;<sub>t</sub></span> (a related quantity), so the network only needs to predict the mean.
      </div>

      <p>And here's where it gets elegant. Remember the forward shortcut formula? We can rewrite the reverse mean in terms of a <em>noise prediction</em>. Instead of asking the network "what did <span class="math">x<sub>t-1</sub></span> look like?", we ask: "what noise <span class="math">&epsilon;</span> was added to create <span class="math">x<sub>t</sub></span>?"</p>

      <p>If the network can predict the noise <span class="math">&epsilon;<sub>&theta;</sub>(x<sub>t</sub>, t)</span>, we can compute the reverse step as:</p>

      <div class="math-block">
        <span class="math">x<sub>t-1</sub> = (1/&radic;&alpha;<sub>t</sub>)(x<sub>t</sub> - (&beta;<sub>t</sub> / &radic;(1 - &alpha;&#772;<sub>t</sub>)) &middot; &epsilon;<sub>&theta;</sub>(x<sub>t</sub>, t)) + &sigma;<sub>t</sub> &middot; z</span>
      </div>

      <p>where <span class="math">&alpha;<sub>t</sub> = 1 - &beta;<sub>t</sub></span>, and <span class="math">z ~ N(0, I)</span> is fresh random noise (except at the final step). The <span class="math">&sigma;<sub>t</sub></span> term controls the stochasticity of sampling &mdash; more on that later.</p>

      <p>Let's see this in action. Below, step backward from noise and watch an image emerge:</p>

      <!-- Interactive: Reverse the Film -->
      <div class="widget" id="reverse-widget">
        <div class="widget__title">Reverse the Film</div>
        <div style="display: flex; gap: 1.5rem; align-items: center; flex-wrap: wrap;">
          <div style="flex: 0 0 auto; text-align: center;">
            <canvas id="rv-canvas" width="200" height="200" style="image-rendering: pixelated; width: 200px; height: 200px; border-radius: 8px; border: 1px solid #ddd;"></canvas>
          </div>
          <div style="flex: 1; min-width: 200px;">
            <div style="margin-bottom: 0.8rem;">
              <label class="widget__title" style="display: block; margin-bottom: 0.3rem;">Timestep <span class="math" id="rv-t-label">t = 100 (noise)</span></label>
              <input type="range" id="rv-slider" min="0" max="100" value="100" style="width: 100%;">
              <div style="display: flex; justify-content: space-between; font-family: 'Figtree', sans-serif; font-size: 0.55rem; color: #999; margin-top: 0.2rem;">
                <span>Clean image (t=0)</span>
                <span>Pure noise (t=100)</span>
              </div>
            </div>
            <div style="display: flex; gap: 0.5rem; margin-top: 0.8rem;">
              <button class="btn btn--primary" id="rv-play-btn">Auto-reverse</button>
              <button class="btn btn--secondary" id="rv-reset-btn">Reset to noise</button>
            </div>
          </div>
        </div>
        <p class="post__figcaption" style="margin-top: 1rem;">Drag the slider from right to left to step backward from noise to image. Or click "Auto-reverse" to watch it happen automatically. This simulates what a trained diffusion model does at generation time.</p>
      </div>

      <p>What you're seeing is a <em>simulation</em> of the reverse process. In a real diffusion model, a neural network would be computing each denoising step. Here, we're using the known original image to compute the "ideal" reverse &mdash; but the principle is the same.</p>
    </div>

    <!-- Section VI: What Does the Network Learn? -->
    <div id="what-network-learns"></div>
    <div class="section-heading">
      <span class="section-heading__number">VI</span>
      <h2>What Does the Network Learn?</h2>
    </div>
    <div class="section-content">
      <p>We said the network predicts the "noise." But there are actually three equivalent ways to frame what the network learns, and they're all mathematically interchangeable:</p>

      <table>
        <thead>
          <tr>
            <th>Prediction target</th>
            <th>What the network outputs</th>
            <th>Intuition</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>Noise prediction</strong> (<span class="math">&epsilon;</span>)</td>
            <td>The noise that was added</td>
            <td>"What's the garbage? I'll subtract it."</td>
          </tr>
          <tr>
            <td><strong>Data prediction</strong> (<span class="math">x<sub>0</sub></span>)</td>
            <td>The original clean image</td>
            <td>"What's hiding under all that noise?"</td>
          </tr>
          <tr>
            <td><strong>Score prediction</strong> (<span class="math">&nabla; log p(x<sub>t</sub>)</span>)</td>
            <td>Direction toward higher probability</td>
            <td>"Which way should I nudge to improve?"</td>
          </tr>
        </tbody>
      </table>

      <p>These are equivalent because of a simple relationship. Given <span class="math">x<sub>t</sub> = &radic;&alpha;&#772;<sub>t</sub> &middot; x<sub>0</sub> + &radic;(1 - &alpha;&#772;<sub>t</sub>) &middot; &epsilon;</span>:</p>

      <ul style="margin: 1rem 0; padding-left: 1.5rem;">
        <li style="margin-bottom: 0.5rem;">If you know <span class="math">&epsilon;</span>, you can solve for <span class="math">x<sub>0</sub></span>: &nbsp; <span class="math">x<sub>0</sub> = (x<sub>t</sub> - &radic;(1 - &alpha;&#772;<sub>t</sub>) &middot; &epsilon;) / &radic;&alpha;&#772;<sub>t</sub></span></li>
        <li style="margin-bottom: 0.5rem;">If you know <span class="math">x<sub>0</sub></span>, you can solve for <span class="math">&epsilon;</span></li>
        <li style="margin-bottom: 0.5rem;">The score is just: &nbsp; <span class="math">&nabla;<sub>x</sub> log p(x<sub>t</sub>) = -&epsilon; / &radic;(1 - &alpha;&#772;<sub>t</sub>)</span></li>
      </ul>

      <p>In practice, <strong>noise prediction</strong> (<span class="math">&epsilon;</span>-prediction) works best and is the most common choice. The network sees a noisy image, and its job is simply: <em>"tell me what the noise looks like, and I'll subtract it."</em></p>

      <p>The architecture of choice is a <strong>U-Net</strong> &mdash; a convolutional neural network with skip connections that preserves spatial detail. The key modification for diffusion models: the timestep <span class="math">t</span> is also fed as input (typically via sinusoidal embeddings), so the network knows <em>how noisy</em> the image is.</p>

      <!-- SVG Diagram: U-Net Architecture -->
      <div class="post__figure">
        <svg viewBox="0 0 680 300" xmlns="http://www.w3.org/2000/svg" style="max-width: 100%; font-family: Figtree, sans-serif;">
          <!-- Background -->
          <rect x="0" y="0" width="680" height="300" fill="none"/>

          <!-- Input -->
          <rect x="20" y="80" width="70" height="140" rx="6" fill="#e0e7ff" stroke="#6366f1" stroke-width="1.5"/>
          <text x="55" y="155" text-anchor="middle" font-size="10" fill="#4338ca" font-weight="600">Noisy</text>
          <text x="55" y="168" text-anchor="middle" font-size="10" fill="#4338ca" font-weight="600">Image</text>
          <text x="55" y="182" text-anchor="middle" font-size="9" fill="#6366f1">x<tspan baseline-shift="sub" font-size="7">t</tspan></text>

          <!-- Encoder blocks -->
          <rect x="120" y="100" width="50" height="100" rx="5" fill="#c7d2fe" stroke="#818cf8" stroke-width="1"/>
          <text x="145" y="145" text-anchor="middle" font-size="8" fill="#4338ca">Conv</text>
          <text x="145" y="157" text-anchor="middle" font-size="8" fill="#4338ca">Down</text>

          <rect x="190" y="120" width="50" height="60" rx="5" fill="#a5b4fc" stroke="#818cf8" stroke-width="1"/>
          <text x="215" y="147" text-anchor="middle" font-size="8" fill="#312e81">Conv</text>
          <text x="215" y="159" text-anchor="middle" font-size="8" fill="#312e81">Down</text>

          <!-- Bottleneck -->
          <rect x="260" y="130" width="60" height="40" rx="5" fill="#6366f1" stroke="#4f46e5" stroke-width="1.5"/>
          <text x="290" y="154" text-anchor="middle" font-size="9" fill="#fff" font-weight="600">Bottleneck</text>

          <!-- Decoder blocks -->
          <rect x="340" y="120" width="50" height="60" rx="5" fill="#a5b4fc" stroke="#818cf8" stroke-width="1"/>
          <text x="365" y="147" text-anchor="middle" font-size="8" fill="#312e81">Conv</text>
          <text x="365" y="159" text-anchor="middle" font-size="8" fill="#312e81">Up</text>

          <rect x="410" y="100" width="50" height="100" rx="5" fill="#c7d2fe" stroke="#818cf8" stroke-width="1"/>
          <text x="435" y="145" text-anchor="middle" font-size="8" fill="#4338ca">Conv</text>
          <text x="435" y="157" text-anchor="middle" font-size="8" fill="#4338ca">Up</text>

          <!-- Output -->
          <rect x="490" y="80" width="70" height="140" rx="6" fill="#fce7f3" stroke="#ec4899" stroke-width="1.5"/>
          <text x="525" y="150" text-anchor="middle" font-size="10" fill="#be185d" font-weight="600">Predicted</text>
          <text x="525" y="163" text-anchor="middle" font-size="10" fill="#be185d" font-weight="600">Noise</text>
          <text x="525" y="177" text-anchor="middle" font-size="9" fill="#ec4899">&epsilon;<tspan baseline-shift="sub" font-size="7">&theta;</tspan></text>

          <!-- Arrows -->
          <line x1="90" y1="150" x2="118" y2="150" stroke="#6366f1" stroke-width="1.5" marker-end="url(#arrowhead)"/>
          <line x1="170" y1="150" x2="188" y2="150" stroke="#818cf8" stroke-width="1.5" marker-end="url(#arrowhead)"/>
          <line x1="240" y1="150" x2="258" y2="150" stroke="#818cf8" stroke-width="1.5" marker-end="url(#arrowhead)"/>
          <line x1="320" y1="150" x2="338" y2="150" stroke="#818cf8" stroke-width="1.5" marker-end="url(#arrowhead)"/>
          <line x1="390" y1="150" x2="408" y2="150" stroke="#818cf8" stroke-width="1.5" marker-end="url(#arrowhead)"/>
          <line x1="460" y1="150" x2="488" y2="150" stroke="#ec4899" stroke-width="1.5" marker-end="url(#arrowpink)"/>

          <!-- Skip connections -->
          <path d="M 145 100 Q 145 60 290 60 Q 365 60 365 120" fill="none" stroke="#f59e0b" stroke-width="1.5" stroke-dasharray="5,3"/>
          <path d="M 215 120 Q 215 90 290 90 Q 435 90 435 100" fill="none" stroke="#f59e0b" stroke-width="1.5" stroke-dasharray="5,3"/>

          <text x="290" y="52" text-anchor="middle" font-size="9" fill="#d97706" font-weight="500">skip connections</text>

          <!-- Timestep input -->
          <rect x="240" y="240" width="100" height="30" rx="15" fill="#fef3c7" stroke="#f59e0b" stroke-width="1"/>
          <text x="290" y="259" text-anchor="middle" font-size="9" fill="#92400e" font-weight="600">Timestep t</text>
          <line x1="290" y1="240" x2="290" y2="172" stroke="#f59e0b" stroke-width="1.5" stroke-dasharray="4,3" marker-end="url(#arrowyellow)"/>

          <!-- Arrow markers -->
          <defs>
            <marker id="arrowhead" markerWidth="8" markerHeight="6" refX="8" refY="3" orient="auto">
              <polygon points="0 0, 8 3, 0 6" fill="#6366f1"/>
            </marker>
            <marker id="arrowpink" markerWidth="8" markerHeight="6" refX="8" refY="3" orient="auto">
              <polygon points="0 0, 8 3, 0 6" fill="#ec4899"/>
            </marker>
            <marker id="arrowyellow" markerWidth="8" markerHeight="6" refX="4" refY="0" orient="auto">
              <polygon points="0 6, 4 0, 8 6" fill="#f59e0b"/>
            </marker>
          </defs>
        </svg>
        <figcaption class="post__figcaption">Simplified U-Net architecture for diffusion models. The noisy image and timestep go in; predicted noise comes out. Skip connections (dashed yellow) let fine details flow from encoder to decoder.</figcaption>
      </div>

      <p>The U-Net architecture is brilliantly suited for this task. The encoder compresses the spatial information, the bottleneck captures global context, and the decoder reconstructs the output &mdash; with skip connections ensuring that fine spatial details aren't lost.</p>

      <div class="sidenote">
        <strong>Why not just a plain CNN?</strong> A plain convolutional network would lose spatial information as it gets deeper. The skip connections in U-Net let the decoder directly access features from the encoder, preserving both high-level semantics <em>and</em> pixel-level detail. This is critical for denoising, where you need to reconstruct fine structures.
      </div>
    </div>

    <!-- Section VII: The Training Loop -->
    <div id="training-loop"></div>
    <div class="section-heading">
      <span class="section-heading__number">VII</span>
      <h2>The Training Loop</h2>
    </div>
    <div class="section-content">
      <p>Here is the entire training algorithm for a diffusion model. It's shockingly simple:</p>

      <div class="widget" style="background: #f0fdf4; border-color: #bbf7d0;">
        <div class="widget__title" style="color: #15803d;">Algorithm: Training a Diffusion Model</div>
        <ol style="padding-left: 1.5rem; font-size: 0.9rem; line-height: 1.7; font-family: 'Figtree', sans-serif;">
          <li><strong>Pick</strong> a clean image <span class="math">x<sub>0</sub></span> from your dataset</li>
          <li><strong>Sample</strong> a random timestep <span class="math">t ~ Uniform(1, T)</span></li>
          <li><strong>Sample</strong> random noise <span class="math">&epsilon; ~ N(0, I)</span></li>
          <li><strong>Compute</strong> the noisy image: <span class="math">x<sub>t</sub> = &radic;&alpha;&#772;<sub>t</sub> &middot; x<sub>0</sub> + &radic;(1 - &alpha;&#772;<sub>t</sub>) &middot; &epsilon;</span></li>
          <li><strong>Feed</strong> <span class="math">x<sub>t</sub></span> and <span class="math">t</span> to the network, get prediction <span class="math">&epsilon;<sub>&theta;</sub>(x<sub>t</sub>, t)</span></li>
          <li><strong>Compute loss</strong>: <span class="math">L = || &epsilon; - &epsilon;<sub>&theta;</sub>(x<sub>t</sub>, t) ||<sup>2</sup></span></li>
          <li><strong>Backprop</strong> and update weights. Repeat.</li>
        </ol>
      </div>

      <p>That's it. The beauty is in the simplicity. There are no adversarial networks fighting each other (as in GANs), no complex reconstruction losses, no mode collapse. Just: add noise, predict it, minimize the difference.</p>

      <div class="sidenote">
        <strong>Why MSE on noise?</strong> This simple loss is actually a weighted variational lower bound (ELBO) on the data log-likelihood. Ho et al. (2020) showed that this simplified loss, which drops certain weighting terms from the ELBO, empirically produces better samples. The mathematical justification runs deep &mdash; it's connected to variational inference, score matching, and denoising autoencoders &mdash; but the training procedure itself remains beautifully simple.
      </div>

      <p>Below, watch a visualization of one training iteration. The animation loops, showing how a network gradually improves its noise predictions:</p>

      <!-- Interactive: Training Playground -->
      <div class="widget" id="training-widget">
        <div class="widget__title">Training Playground</div>
        <div style="display: flex; gap: 1rem; flex-wrap: wrap; align-items: flex-start;">
          <div style="flex: 1; min-width: 130px; text-align: center;">
            <div class="widget__title">Clean x<sub>0</sub></div>
            <canvas id="tr-clean" width="100" height="100" style="image-rendering: pixelated; width: 100px; height: 100px; border-radius: 6px; border: 1px solid #ddd;"></canvas>
          </div>
          <div style="flex: 0 0 auto; display: flex; align-items: center; font-size: 1.2rem; color: #999; padding-top: 1.5rem;">+&epsilon;&rarr;</div>
          <div style="flex: 1; min-width: 130px; text-align: center;">
            <div class="widget__title">Noisy x<sub>t</sub></div>
            <canvas id="tr-noisy" width="100" height="100" style="image-rendering: pixelated; width: 100px; height: 100px; border-radius: 6px; border: 1px solid #ddd;"></canvas>
            <div style="font-family: 'Figtree', sans-serif; font-size: 0.55rem; color: #999; margin-top: 0.2rem;" id="tr-timestep">t = ???</div>
          </div>
          <div style="flex: 0 0 auto; display: flex; align-items: center; font-size: 1.2rem; color: #999; padding-top: 1.5rem;">&rarr;</div>
          <div style="flex: 1; min-width: 130px; text-align: center;">
            <div class="widget__title">Predicted &epsilon;&#770;</div>
            <canvas id="tr-predicted" width="100" height="100" style="image-rendering: pixelated; width: 100px; height: 100px; border-radius: 6px; border: 1px solid #ddd;"></canvas>
          </div>
        </div>
        <div style="margin-top: 1rem;">
          <div class="widget__title">Training Loss (MSE)</div>
          <canvas id="tr-loss-chart" width="400" height="80" style="width: 100%; height: 80px; border-radius: 6px; border: 1px solid #ddd;"></canvas>
          <div style="display: flex; justify-content: space-between; font-family: 'Figtree', sans-serif; font-size: 0.55rem; color: #999; margin-top: 0.2rem;">
            <span>Epoch 0</span>
            <span id="tr-epoch-label">Epoch 0</span>
          </div>
        </div>
        <div style="margin-top: 0.8rem; display: flex; gap: 0.5rem;">
          <button class="btn btn--primary" id="tr-play-btn">Train!</button>
          <button class="btn btn--secondary" id="tr-reset-btn">Reset</button>
        </div>
        <p class="post__figcaption" style="margin-top: 0.8rem;">Watch the network learn to predict noise. The loss chart shows MSE decreasing as training progresses. This is a simplified visualization &mdash; real training uses millions of images and thousands of epochs.</p>
      </div>

      <p>A few practical notes about training:</p>

      <ul style="margin: 1rem 0; padding-left: 1.5rem;">
        <li style="margin-bottom: 0.5rem;"><strong>Random timesteps</strong>: Each training step samples a <em>random</em> <span class="math">t</span>. This means the network learns to denoise at <em>all</em> noise levels simultaneously.</li>
        <li style="margin-bottom: 0.5rem;"><strong>Timestep conditioning</strong>: The network receives <span class="math">t</span> as input (via sinusoidal embeddings, similar to transformer position encodings). This tells it how much noise to expect.</li>
        <li style="margin-bottom: 0.5rem;"><strong>Scale</strong>: Stable Diffusion was trained on billions of image-text pairs. But the <em>algorithm</em> is the same simple loop above.</li>
      </ul>

      <div class="sidenote">
        <strong>Diffusion vs GANs:</strong> GANs are notoriously hard to train &mdash; the generator and discriminator can destabilize each other, leading to mode collapse (generating only a few types of images) or training divergence. Diffusion models train with a simple MSE loss on a single network. This makes them far more stable, though they're slower at generation time (many steps vs one forward pass for GANs).
      </div>
    </div>

    <!-- Section VIII: Sampling -->
    <div id="sampling"></div>
    <div class="section-heading">
      <span class="section-heading__number">VIII</span>
      <h2>Sampling &mdash; Birth of an Image</h2>
    </div>
    <div class="section-content">
      <p>Once trained, generating an image is the reverse process in action:</p>

      <div class="widget" style="background: #faf5ff; border-color: #e9d5ff;">
        <div class="widget__title" style="color: #7c3aed;">Algorithm: Sampling (DDPM)</div>
        <ol style="padding-left: 1.5rem; font-size: 0.9rem; line-height: 1.7; font-family: 'Figtree', sans-serif;">
          <li><strong>Start</strong> with pure noise: <span class="math">x<sub>T</sub> ~ N(0, I)</span></li>
          <li><strong>For</strong> <span class="math">t = T, T-1, ..., 1</span>:
            <ul style="margin: 0.3rem 0; padding-left: 1.2rem;">
              <li>Predict the noise: <span class="math">&epsilon;&#770; = &epsilon;<sub>&theta;</sub>(x<sub>t</sub>, t)</span></li>
              <li>Sample <span class="math">z ~ N(0, I)</span> if <span class="math">t > 1</span>, else <span class="math">z = 0</span></li>
              <li>Compute: <span class="math">x<sub>t-1</sub> = (1/&radic;&alpha;<sub>t</sub>)(x<sub>t</sub> - (&beta;<sub>t</sub>/&radic;(1-&alpha;&#772;<sub>t</sub>)) &middot; &epsilon;&#770;) + &sigma;<sub>t</sub> &middot; z</span></li>
            </ul>
          </li>
          <li><strong>Return</strong> <span class="math">x<sub>0</sub></span></li>
        </ol>
      </div>

      <p>Each step takes the current noisy image, asks the network "what noise do you see?", subtracts most of it, and adds back a small amount of fresh noise (the <span class="math">&sigma;<sub>t</sub> &middot; z</span> term). That last bit of re-randomization is important &mdash; it's what makes the process <em>stochastic</em> and allows the model to generate diverse outputs.</p>

      <p>The <span class="math">z</span> term is why different random seeds give different images from the same model. And at the very last step (<span class="math">t = 1</span>), we skip the noise addition to get a clean final result.</p>

      <p>Try it below. Click "Generate" to watch noise gradually resolve into a recognizable shape:</p>

      <!-- Interactive: The Generator -->
      <div class="widget" id="generator-widget">
        <div class="widget__title">The Generator</div>
        <div style="display: flex; gap: 1.5rem; align-items: flex-start; flex-wrap: wrap;">
          <div style="flex: 0 0 auto; text-align: center;">
            <canvas id="gen-canvas" width="200" height="200" style="image-rendering: pixelated; width: 200px; height: 200px; border-radius: 8px; border: 1px solid #ddd;"></canvas>
            <div style="font-family: 'Figtree', sans-serif; font-size: 0.55rem; color: #999; margin-top: 0.3rem;" id="gen-step-label">Ready</div>
          </div>
          <div style="flex: 1; min-width: 200px;">
            <div style="margin-bottom: 0.8rem;">
              <div class="widget__title">Denoising Steps</div>
              <div style="display: flex; gap: 4px; flex-wrap: wrap;" id="gen-filmstrip"></div>
            </div>
            <div style="display: flex; gap: 0.5rem; flex-wrap: wrap;">
              <button class="btn btn--primary" id="gen-go-btn">Generate!</button>
              <button class="btn btn--secondary" id="gen-seed-btn">New Seed</button>
            </div>
            <div style="margin-top: 0.8rem;">
              <label class="widget__title" style="display: block; margin-bottom: 0.3rem;">Speed</label>
              <input type="range" id="gen-speed" min="1" max="10" value="5" style="width: 100%;">
            </div>
          </div>
        </div>
        <p class="post__figcaption" style="margin-top: 1rem;">Click "Generate" to start from noise and walk backward to an image. The filmstrip shows snapshots along the way. Try "New Seed" for different results from the same model.</p>
      </div>

      <p>Notice how the image starts as pure static, then vague structure emerges, then finer details fill in. This coarse-to-fine progression is characteristic of diffusion models &mdash; global structure (is it a face? a landscape?) is determined in the early steps, while fine details (texture, edges) are refined in the later steps.</p>
    </div>

    <!-- Section IX: The Score Function Perspective -->
    <div id="score-function"></div>
    <div class="section-heading">
      <span class="section-heading__number">IX</span>
      <h2>The Score Function Perspective</h2>
    </div>
    <div class="section-content">
      <p>There's an alternative &mdash; and beautiful &mdash; way to understand what diffusion models are doing. It comes from a field called <em>score-based generative modeling</em>, and it provides deep insight into <em>why</em> diffusion models work at all.</p>

      <p>The <strong>score function</strong> of a distribution <span class="math">p(x)</span> is the gradient of the log-probability:</p>

      <div class="math-block">
        <span class="math">s(x) = &nabla;<sub>x</sub> log p(x)</span>
      </div>

      <p>Think of it as an arrow at every point in space, pointing in the direction of "more probable data." In a region of high probability (near real images), the arrows are small. In low-probability regions (noise), the arrows point strongly toward the data.</p>

      <p>If you had the score function everywhere, you could generate samples using <strong>Langevin dynamics</strong>: start at a random point and follow the score (plus a bit of noise) until you arrive at a high-probability region:</p>

      <div class="math-block">
        <span class="math">x<sub>i+1</sub> = x<sub>i</sub> + (&eta;/2) &middot; &nabla;<sub>x</sub> log p(x<sub>i</sub>) + &radic;&eta; &middot; z<sub>i</sub></span>
      </div>

      <p>This is remarkably similar to the diffusion sampling process &mdash; and that's not a coincidence.</p>

      <!-- Interactive: Score Vector Field -->
      <div class="widget" id="score-widget">
        <div class="widget__title">Score Vector Field</div>
        <div style="text-align: center;">
          <canvas id="score-canvas" width="400" height="300" style="width: 100%; max-width: 500px; height: auto; border-radius: 8px; border: 1px solid #ddd;"></canvas>
        </div>
        <div style="margin-top: 0.8rem; display: flex; gap: 0.5rem; justify-content: center; flex-wrap: wrap;">
          <button class="btn btn--primary" id="score-sample-btn">Sample a particle!</button>
          <button class="btn btn--secondary" id="score-reset-btn">Reset</button>
        </div>
        <p class="post__figcaption" style="margin-top: 0.8rem;">A 2D illustration. The purple blobs are "data" (high probability). The arrows show the score &mdash; they point toward data from everywhere. Click "Sample a particle" to watch a random point follow the score function into a data cluster.</p>
      </div>

      <p>The connection to noise prediction is direct. Recall that the score at noise level <span class="math">t</span> is:</p>

      <div class="math-block">
        <span class="math">&nabla;<sub>x</sub> log p(x<sub>t</sub>) = -&epsilon; / &radic;(1 - &alpha;&#772;<sub>t</sub>)</span>
      </div>

      <p>So a noise-predicting network <em>is</em> a score estimator (up to scaling). When the network predicts "the noise is pointing this way," it's equivalently saying "real data is <em>that</em> way." The entire diffusion sampling process is gradient ascent on the data log-likelihood, performed at progressively finer noise scales.</p>

      <div class="sidenote">
        <strong>Why multiple noise scales?</strong> Estimating the score in low-density regions (far from any data) is hard &mdash; there aren't many training examples there. But at high noise levels, the data distribution is spread out, making the score well-defined everywhere. By starting at high noise (blurry score) and progressively reducing noise (sharper score), diffusion models solve the problem that bedeviled earlier score-based methods.
      </div>
    </div>

    <!-- Section X: Variance Schedules -->
    <div id="variance-schedules"></div>
    <div class="section-heading">
      <span class="section-heading__number">X</span>
      <h2>Variance Schedules &amp; Noise Levels</h2>
    </div>
    <div class="section-content">
      <p>The noise schedule <span class="math">&beta;<sub>1</sub>, &beta;<sub>2</sub>, ..., &beta;<sub>T</sub></span> is one of the most important design choices in a diffusion model. It determines how quickly information is destroyed &mdash; and how smoothly the model can learn to reverse the process.</p>

      <p><strong>Linear schedule</strong> (original DDPM): <span class="math">&beta;<sub>t</sub></span> increases linearly from <span class="math">&beta;<sub>1</sub> = 10<sup>-4</sup></span> to <span class="math">&beta;<sub>T</sub> = 0.02</span>. Simple but flawed &mdash; it destroys information too quickly in the middle timesteps.</p>

      <p><strong>Cosine schedule</strong> (Improved DDPM): Defines <span class="math">&alpha;&#772;<sub>t</sub></span> to follow a cosine curve, ensuring a smoother decay of signal-to-noise ratio. Information is preserved longer, giving the model more to work with at intermediate timesteps.</p>

      <!-- Interactive: Schedule Designer -->
      <div class="widget" id="schedule-widget">
        <div class="widget__title">Schedule Comparison</div>
        <div style="display: flex; gap: 0.8rem; margin-bottom: 1rem;">
          <button class="btn btn--primary sched-btn" data-sched="cosine">Cosine</button>
          <button class="btn btn--secondary sched-btn" data-sched="linear">Linear</button>
          <button class="btn btn--secondary sched-btn" data-sched="sigmoid">Sigmoid</button>
        </div>
        <canvas id="sched-canvas" width="500" height="200" style="width: 100%; height: 200px; border-radius: 8px; border: 1px solid #ddd;"></canvas>
        <div style="display: flex; justify-content: space-between; font-family: 'Figtree', sans-serif; font-size: 0.55rem; color: #999; margin-top: 0.2rem;">
          <span>t = 0</span>
          <span>t = T</span>
        </div>
        <div style="display: flex; gap: 2rem; margin-top: 0.8rem; flex-wrap: wrap;">
          <div style="display: flex; align-items: center; gap: 0.4rem; font-family: 'Figtree', sans-serif; font-size: 0.65rem;">
            <div style="width: 12px; height: 12px; background: #6366f1; border-radius: 3px;"></div>
            <span>&alpha;&#772;<sub>t</sub> (signal)</span>
          </div>
          <div style="display: flex; align-items: center; gap: 0.4rem; font-family: 'Figtree', sans-serif; font-size: 0.65rem;">
            <div style="width: 12px; height: 12px; background: #ef4444; border-radius: 3px;"></div>
            <span>1 - &alpha;&#772;<sub>t</sub> (noise)</span>
          </div>
          <div style="display: flex; align-items: center; gap: 0.4rem; font-family: 'Figtree', sans-serif; font-size: 0.65rem;">
            <div style="width: 12px; height: 12px; background: #f59e0b; border-radius: 3px;"></div>
            <span>SNR (log scale)</span>
          </div>
        </div>
        <p class="post__figcaption" style="margin-top: 0.8rem;">Compare how different schedules control the signal-to-noise ratio. The cosine schedule provides a smoother, more gradual transition than linear.</p>
      </div>

      <p>The cosine schedule has become the default for most modern diffusion models because it avoids the "information cliff" of the linear schedule &mdash; a region where the SNR drops precipitously, making learning harder.</p>
    </div>

    <!-- Section XI: Speeding Things Up -->
    <div id="speeding-up"></div>
    <div class="section-heading">
      <span class="section-heading__number">XI</span>
      <h2>Speeding Things Up</h2>
    </div>
    <div class="section-content">
      <p>DDPM's main weakness: sampling requires 1000 forward passes through the U-Net. That's slow. A single image might need 10&ndash;30 seconds on a GPU. Can we do better?</p>

      <p><strong>DDIM</strong> (Denoising Diffusion Implicit Models) was the first breakthrough. The key insight: make the reverse process <em>deterministic</em> by removing the random noise term <span class="math">&sigma;<sub>t</sub> &middot; z</span>. Without randomness, you can skip steps &mdash; jumping from <span class="math">t = 1000</span> to <span class="math">t = 950</span> directly, instead of stepping through every integer.</p>

      <p>This reduces sampling to 20&ndash;50 steps with minimal quality loss. The tradeoff: deterministic sampling means the same seed always gives the same output (which is often <em>desirable</em>).</p>

      <!-- Interactive: Speed vs Quality -->
      <div class="widget" id="speed-widget">
        <div class="widget__title">Speed vs Quality</div>
        <div style="display: flex; gap: 6px; flex-wrap: wrap; justify-content: center;" id="speed-gallery"></div>
        <div style="margin-top: 1rem;">
          <label class="widget__title" style="display: block; margin-bottom: 0.3rem;">Number of denoising steps: <span id="speed-steps-label">50</span></label>
          <input type="range" id="speed-slider" min="1" max="100" value="50" style="width: 100%;">
          <div style="display: flex; justify-content: space-between; font-family: 'Figtree', sans-serif; font-size: 0.55rem; color: #999; margin-top: 0.2rem;">
            <span>1 step (fast, bad)</span>
            <span>100 steps (slow, good)</span>
          </div>
        </div>
        <p class="post__figcaption" style="margin-top: 0.8rem;">Fewer steps means faster generation but more artifacts. Real models produce stunning results at 20&ndash;50 steps.</p>
      </div>

      <p>The race to reduce steps has continued:</p>

      <ul style="margin: 1rem 0; padding-left: 1.5rem;">
        <li style="margin-bottom: 0.5rem;"><strong>DPM-Solver</strong>: Uses higher-order ODE solvers for faster convergence (10&ndash;20 steps)</li>
        <li style="margin-bottom: 0.5rem;"><strong>Consistency Models</strong>: Map any noise level directly to the final image in a single step</li>
        <li style="margin-bottom: 0.5rem;"><strong>Rectified Flows</strong>: Learn straight-line paths from noise to data, enabling few-step generation</li>
        <li style="margin-bottom: 0.5rem;"><strong>Distillation</strong>: Train a student model to mimic a multi-step teacher in fewer steps</li>
      </ul>

      <p>Today's best models can produce high-quality images in 1&ndash;4 steps. The gap between "slow but beautiful" and "fast but ugly" has almost completely closed.</p>
    </div>

    <!-- Section XII: From Pixels to Latent Space -->
    <div id="latent-space"></div>
    <div class="section-heading">
      <span class="section-heading__number">XII</span>
      <h2>From Pixels to Latent Space</h2>
    </div>
    <div class="section-content">
      <p>Everything we've discussed so far operates directly on pixel values. But a 512&times;512 color image has 786,432 dimensions. Running a U-Net on that is expensive.</p>

      <p>The key innovation of <strong>Latent Diffusion</strong> (the architecture behind Stable Diffusion): don't do diffusion in pixel space. Instead:</p>

      <ol style="margin: 1rem 0; padding-left: 1.5rem;">
        <li style="margin-bottom: 0.5rem;"><strong>Compress</strong> the image to a much smaller latent representation using a pre-trained VAE (Variational Autoencoder)</li>
        <li style="margin-bottom: 0.5rem;"><strong>Do diffusion</strong> in this compact latent space</li>
        <li style="margin-bottom: 0.5rem;"><strong>Decode</strong> back to pixel space using the VAE decoder</li>
      </ol>

      <p>A 512&times;512 image might compress to a 64&times;64&times;4 latent &mdash; that's a 48x reduction in dimensionality. The diffusion U-Net is dramatically cheaper to run.</p>

      <!-- SVG Diagram: Latent Diffusion Pipeline -->
      <div class="post__figure">
        <svg viewBox="0 0 680 260" xmlns="http://www.w3.org/2000/svg" style="max-width: 100%; font-family: Figtree, sans-serif;">
          <!-- Text prompt -->
          <rect x="10" y="10" width="120" height="40" rx="20" fill="#fef3c7" stroke="#f59e0b" stroke-width="1.5"/>
          <text x="70" y="35" text-anchor="middle" font-size="11" fill="#92400e" font-weight="600">"a sunset"</text>

          <!-- CLIP -->
          <rect x="10" y="75" width="120" height="35" rx="6" fill="#fed7aa" stroke="#f97316" stroke-width="1"/>
          <text x="70" y="97" text-anchor="middle" font-size="10" fill="#9a3412" font-weight="600">CLIP Encoder</text>
          <line x1="70" y1="50" x2="70" y2="75" stroke="#f59e0b" stroke-width="1.5" marker-end="url(#arrowhead)"/>

          <!-- Cross attention arrow -->
          <path d="M 130 92 Q 200 92 235 150" fill="none" stroke="#f97316" stroke-width="1.5" stroke-dasharray="5,3" marker-end="url(#arroworange)"/>
          <text x="185" y="115" text-anchor="middle" font-size="8" fill="#ea580c">cross-attention</text>

          <!-- VAE Encoder -->
          <rect x="160" y="190" width="80" height="50" rx="6" fill="#dcfce7" stroke="#22c55e" stroke-width="1.5"/>
          <text x="200" y="212" text-anchor="middle" font-size="9" fill="#15803d" font-weight="600">VAE</text>
          <text x="200" y="225" text-anchor="middle" font-size="9" fill="#15803d" font-weight="600">Encoder</text>

          <!-- Image input -->
          <rect x="160" y="240" width="80" height="15" rx="3" fill="#e0e7ff"/>
          <text x="200" y="252" text-anchor="middle" font-size="7" fill="#6366f1">512&times;512 image</text>

          <!-- Latent space -->
          <rect x="270" y="130" width="140" height="70" rx="8" fill="#ede9fe" stroke="#8b5cf6" stroke-width="2"/>
          <text x="340" y="158" text-anchor="middle" font-size="11" fill="#6d28d9" font-weight="700">Diffusion U-Net</text>
          <text x="340" y="175" text-anchor="middle" font-size="8" fill="#7c3aed">in 64&times;64 latent space</text>
          <text x="340" y="190" text-anchor="middle" font-size="8" fill="#7c3aed">(noise &harr; denoise)</text>

          <line x1="240" y1="215" x2="270" y2="180" stroke="#22c55e" stroke-width="1.5" marker-end="url(#arrowgreen)"/>

          <!-- VAE Decoder -->
          <rect x="440" y="190" width="80" height="50" rx="6" fill="#dcfce7" stroke="#22c55e" stroke-width="1.5"/>
          <text x="480" y="212" text-anchor="middle" font-size="9" fill="#15803d" font-weight="600">VAE</text>
          <text x="480" y="225" text-anchor="middle" font-size="9" fill="#15803d" font-weight="600">Decoder</text>

          <line x1="410" y1="180" x2="440" y2="215" stroke="#8b5cf6" stroke-width="1.5" marker-end="url(#arrowgreen)"/>

          <!-- Output -->
          <rect x="550" y="190" width="110" height="50" rx="8" fill="#fce7f3" stroke="#ec4899" stroke-width="1.5"/>
          <text x="605" y="212" text-anchor="middle" font-size="10" fill="#be185d" font-weight="600">Generated</text>
          <text x="605" y="226" text-anchor="middle" font-size="10" fill="#be185d" font-weight="600">Image</text>

          <line x1="520" y1="215" x2="548" y2="215" stroke="#22c55e" stroke-width="1.5" marker-end="url(#arrowpink)"/>

          <!-- Image output label -->
          <rect x="550" y="240" width="110" height="15" rx="3" fill="#fce7f3"/>
          <text x="605" y="252" text-anchor="middle" font-size="7" fill="#ec4899">512&times;512 output</text>

          <!-- Noise input -->
          <rect x="270" y="80" width="60" height="28" rx="14" fill="#f5f5f5" stroke="#999" stroke-width="1"/>
          <text x="300" y="99" text-anchor="middle" font-size="9" fill="#666" font-weight="500">Noise z</text>
          <line x1="300" y1="108" x2="300" y2="130" stroke="#999" stroke-width="1.5" marker-end="url(#arrowgray)"/>

          <!-- Arrow markers -->
          <defs>
            <marker id="arroworange" markerWidth="8" markerHeight="6" refX="8" refY="3" orient="auto">
              <polygon points="0 0, 8 3, 0 6" fill="#f97316"/>
            </marker>
            <marker id="arrowgreen" markerWidth="8" markerHeight="6" refX="8" refY="3" orient="auto">
              <polygon points="0 0, 8 3, 0 6" fill="#22c55e"/>
            </marker>
            <marker id="arrowgray" markerWidth="8" markerHeight="6" refX="4" refY="0" orient="auto">
              <polygon points="0 6, 4 0, 8 6" fill="#999"/>
            </marker>
          </defs>
        </svg>
        <figcaption class="post__figcaption">The Stable Diffusion pipeline. A text prompt is encoded by CLIP and injected via cross-attention. Diffusion happens in a compact latent space (64&times;64), then the VAE decoder reconstructs the full-resolution image.</figcaption>
      </div>

      <h3>Text Conditioning &amp; Classifier-Free Guidance</h3>

      <p>How does a text prompt steer image generation? The text is encoded by a language model (CLIP), producing a sequence of embedding vectors. These embeddings are fed into the U-Net via <strong>cross-attention layers</strong> &mdash; at each spatial location, the network can "attend to" relevant parts of the prompt.</p>

      <p>But conditioning alone isn't enough. <strong>Classifier-free guidance (CFG)</strong> is the trick that makes text-to-image models actually follow prompts:</p>

      <div class="math-block">
        <span class="math">&epsilon;&#770;<sub>guided</sub> = &epsilon;&#770;<sub>uncond</sub> + w &middot; (&epsilon;&#770;<sub>cond</sub> - &epsilon;&#770;<sub>uncond</sub>)</span>
      </div>

      <p>During training, the text condition is randomly dropped (replaced with null) some percentage of the time. At inference, the model makes <em>two</em> predictions: one with the prompt, one without. The difference is amplified by a guidance scale <span class="math">w</span> (typically 7&ndash;15). Higher <span class="math">w</span> means stronger prompt adherence but less diversity.</p>

      <div class="sidenote">
        <strong>The guidance scale tradeoff:</strong> At <span class="math">w = 1</span>, you get the raw conditional model &mdash; diverse but often ignoring the prompt. At <span class="math">w = 15</span>, images closely match the text but look more "generic." Most users settle around <span class="math">w = 7.5</span>. This tradeoff between fidelity and diversity is a fundamental property of guided generation.
      </div>
    </div>

    <!-- Section XIII: Further Resources -->
    <div id="further-resources"></div>
    <div class="section-heading">
      <span class="section-heading__number">XIII</span>
      <h2>Further Resources</h2>
    </div>
    <div class="section-content">
      <p>If you've made it this far, you understand diffusion models better than most people who use them every day. Here are some resources for going deeper:</p>

      <ul style="margin: 1rem 0; padding-left: 1.5rem;">
        <li style="margin-bottom: 0.8rem;">
          <strong>The DDPM Paper</strong> (Ho et al., 2020). "Denoising Diffusion Probabilistic Models" &mdash; the paper that started the modern diffusion era. Clean writing, clear math, and the training algorithm we derived above.
        </li>
        <li style="margin-bottom: 0.8rem;">
          <strong>Score-Based Generative Modeling</strong> (Song &amp; Ermon, 2019). "Generative Modeling by Estimating Gradients of the Data Distribution" &mdash; the score-based perspective that gave us deep insights into why diffusion works.
        </li>
        <li style="margin-bottom: 0.8rem;">
          <strong>Lilian Weng's blog post</strong>: "What are Diffusion Models?" &mdash; one of the best technical explanations with thorough math derivations.
        </li>
        <li style="margin-bottom: 0.8rem;">
          <strong>The Latent Diffusion Paper</strong> (Rombach et al., 2022). "High-Resolution Image Synthesis with Latent Diffusion Models" &mdash; the architecture behind Stable Diffusion.
        </li>
        <li style="margin-bottom: 0.8rem;">
          <strong>Calvin Luo's tutorial</strong>: "Understanding Diffusion Models: A Unified Perspective" &mdash; a comprehensive mathematical treatment connecting all the different perspectives.
        </li>
      </ul>

      <p style="margin-top: 2rem; font-style: italic; color: #666;">This explainer covers the fundamental principles. Real production models add many engineering details &mdash; EMA, attention, adaptive normalization, progressive training &mdash; but the core ideas are exactly what you've just learned. If you can explain the forward process, the training loop, and the sampling algorithm from memory, you've got it.</p>
    </div>

  </article>

  <!-- FOOTER -->
  <footer class="site-footer">
    <div class="site-footer__tagline">
      You don't understand it until you can derive it on a napkin.
    </div>
    <div class="site-footer__bottom">
      Diffusion Models from First Principles &mdash; 2025
    </div>
  </footer>

  <script>
    // ===== HERO PARTICLE ANIMATION =====
    (function() {
      const container = document.getElementById('heroParticles');
      const canvas = document.createElement('canvas');
      canvas.style.width = '100%';
      canvas.style.height = '100%';
      canvas.style.position = 'absolute';
      canvas.style.top = '0';
      canvas.style.left = '0';
      container.appendChild(canvas);

      const ctx = canvas.getContext('2d');
      let particles = [];
      let animId;

      function resize() {
        canvas.width = container.offsetWidth * 2;
        canvas.height = container.offsetHeight * 2;
        canvas.style.width = container.offsetWidth + 'px';
        canvas.style.height = container.offsetHeight + 'px';
        ctx.scale(2, 2);
      }

      function initParticles() {
        particles = [];
        const w = container.offsetWidth;
        const h = container.offsetHeight;
        for (let i = 0; i < 60; i++) {
          particles.push({
            x: Math.random() * w,
            y: Math.random() * h,
            r: Math.random() * 2 + 1,
            vx: (Math.random() - 0.5) * 0.5,
            vy: (Math.random() - 0.5) * 0.5,
            opacity: Math.random() * 0.4 + 0.1
          });
        }
      }

      function draw() {
        const w = container.offsetWidth;
        const h = container.offsetHeight;
        ctx.clearRect(0, 0, w, h);

        for (const p of particles) {
          p.x += p.vx;
          p.y += p.vy;
          if (p.x < 0) p.x = w;
          if (p.x > w) p.x = 0;
          if (p.y < 0) p.y = h;
          if (p.y > h) p.y = 0;

          ctx.beginPath();
          ctx.arc(p.x, p.y, p.r, 0, Math.PI * 2);
          ctx.fillStyle = `rgba(255,255,255,${p.opacity})`;
          ctx.fill();
        }
        animId = requestAnimationFrame(draw);
      }

      resize();
      initParticles();
      draw();
      window.addEventListener('resize', () => { resize(); initParticles(); });
    })();

    // ===== SHARED UTILITIES =====
    function seededRNG(seed) {
      return function() {
        seed |= 0; seed = seed + 0x6D2B79F5 | 0;
        let t = Math.imul(seed ^ seed >>> 15, 1 | seed);
        t = t + Math.imul(t ^ t >>> 7, 61 | t) ^ t;
        return ((t ^ t >>> 14) >>> 0) / 4294967296;
      };
    }

    function gaussianFromRNG(rng) {
      let u1 = rng(), u2 = rng();
      while (u1 === 0) u1 = rng();
      return Math.sqrt(-2 * Math.log(u1)) * Math.cos(2 * Math.PI * u2);
    }

    // Shape generators (8x8 pixel grids)
    const SHAPES = {
      smiley: [
        [0,0,0,0,0,0,0,0],[0,0,1,0,0,1,0,0],[0,0,1,0,0,1,0,0],[0,0,0,0,0,0,0,0],
        [0,1,0,0,0,0,1,0],[0,0,1,0,0,1,0,0],[0,0,0,1,1,0,0,0],[0,0,0,0,0,0,0,0]
      ],
      star: [
        [0,0,0,1,0,0,0,0],[0,0,0,1,0,0,0,0],[0,1,1,1,1,1,0,0],[0,0,1,1,1,0,0,0],
        [0,1,1,0,1,1,0,0],[0,1,0,0,0,1,0,0],[0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0]
      ],
      heart: [
        [0,0,0,0,0,0,0,0],[0,1,1,0,1,1,0,0],[1,1,1,1,1,1,1,0],[1,1,1,1,1,1,1,0],
        [0,1,1,1,1,1,0,0],[0,0,1,1,1,0,0,0],[0,0,0,1,0,0,0,0],[0,0,0,0,0,0,0,0]
      ]
    };

    const SHAPE_COLORS = {
      smiley: { bg: [255, 220, 60], fg: [60, 40, 20] },
      star: { bg: [30, 30, 60], fg: [255, 215, 0] },
      heart: { bg: [255, 240, 245], fg: [220, 40, 80] }
    };

    function getShapePixels(name) {
      const grid = SHAPES[name];
      const colors = SHAPE_COLORS[name];
      const pixels = [];
      for (let y = 0; y < 8; y++) {
        for (let x = 0; x < 8; x++) {
          pixels.push(grid[y][x] === 1 ? [...colors.fg] : [...colors.bg]);
        }
      }
      return pixels;
    }

    function cosineAlphaBar(t, T) {
      const s = 0.008;
      const f = (tt) => Math.cos(((tt / T) + s) / (1 + s) * Math.PI / 2) ** 2;
      return f(t) / f(0);
    }

    function renderPixelGrid(ctx, pixels, size, canvasW) {
      const pxSize = canvasW / size;
      for (let y = 0; y < size; y++) {
        for (let x = 0; x < size; x++) {
          const [r, g, b] = pixels[y * size + x];
          ctx.fillStyle = `rgb(${Math.round(r)},${Math.round(g)},${Math.round(b)})`;
          ctx.fillRect(x * pxSize, y * pxSize, pxSize, pxSize);
        }
      }
    }

    // Returns pixels clipped to [0,255] for rendering
    function noiseImage(cleanPixels, noiseVecs, t, T) {
      const ab = cosineAlphaBar(t, T);
      const sqA = Math.sqrt(ab);
      const sqNA = Math.sqrt(1 - ab);
      return cleanPixels.map((px, i) => {
        const n = noiseVecs[i];
        return [
          Math.max(0, Math.min(255, sqA * px[0] + sqNA * n[0] * 128)),
          Math.max(0, Math.min(255, sqA * px[1] + sqNA * n[1] * 128)),
          Math.max(0, Math.min(255, sqA * px[2] + sqNA * n[2] * 128))
        ];
      });
    }

    // Returns RAW unclipped values in normalized [-1,1] space for histogram
    function noiseImageRaw(cleanPixels, noiseVecs, t, T) {
      const ab = cosineAlphaBar(t, T);
      const sqA = Math.sqrt(ab);
      const sqNA = Math.sqrt(1 - ab);
      return cleanPixels.map((px, i) => {
        const n = noiseVecs[i];
        // Normalize pixel from [0,255] to [-1,1], apply noise, keep in [-1,1] space
        return [
          sqA * (px[0] / 127.5 - 1) + sqNA * n[0],
          sqA * (px[1] / 127.5 - 1) + sqNA * n[1],
          sqA * (px[2] / 127.5 - 1) + sqNA * n[2]
        ];
      });
    }

    function generateNoiseVecs(count, seed) {
      const rng = seededRNG(seed);
      const vecs = [];
      for (let i = 0; i < count; i++) {
        vecs.push([gaussianFromRNG(rng), gaussianFromRNG(rng), gaussianFromRNG(rng)]);
      }
      return vecs;
    }

    // ===== DESTRUCTION FILM INTERACTIVE =====
    (function() {
      const canvas = document.getElementById('destruction-canvas');
      const ctx = canvas.getContext('2d');
      const slider = document.getElementById('destruction-slider');
      const tLabel = document.getElementById('destruction-t-label');
      const signalBar = document.getElementById('destruction-signal-bar');
      const noiseBar = document.getElementById('destruction-noise-bar');

      const cleanPixels = getShapePixels('smiley');
      const noiseVecs = generateNoiseVecs(64, 42);

      function renderFrame(t) {
        const ab = cosineAlphaBar(t, 100);
        const noisyPx = noiseImage(cleanPixels, noiseVecs, t, 100);
        renderPixelGrid(ctx, noisyPx, 8, canvas.width);
        tLabel.textContent = `t = ${t}`;
        signalBar.style.width = `${ab * 100}%`;
        noiseBar.style.width = `${(1 - ab) * 100}%`;
      }

      slider.addEventListener('input', () => renderFrame(parseInt(slider.value)));
      renderFrame(0);
    })();

    // ===== NOISE KITCHEN INTERACTIVE =====
    (function() {
      const canvas = document.getElementById('nk-canvas');
      const ctx = canvas.getContext('2d');
      const histCanvas = document.getElementById('nk-histogram');
      const histCtx = histCanvas.getContext('2d');
      const slider = document.getElementById('nk-slider');
      const tLabel = document.getElementById('nk-t-label');

      let currentShape = 'smiley';
      let cleanPixels = getShapePixels(currentShape);
      const noiseVecs = generateNoiseVecs(64, 123);
      // Extra noise vecs for histogram (many more samples for smooth distribution)
      const histNoiseVecs = generateNoiseVecs(2000, 456);

      function drawHistogram(cleanPx, t) {
        const w = histCanvas.width;
        const h = histCanvas.height;
        histCtx.clearRect(0, 0, w, h);

        // Use many extra samples for a smooth histogram
        // Pick a representative clean pixel from the shape
        const samplePixels = [];
        for (let i = 0; i < 2000; i++) {
          samplePixels.push(cleanPx[i % cleanPx.length]);
        }
        const rawPx = noiseImageRaw(samplePixels, histNoiseVecs, t, 1000);

        // Collect all channel values (already in normalized space)
        const vals = [];
        for (const px of rawPx) {
          vals.push(px[0]);
          vals.push(px[1]);
          vals.push(px[2]);
        }

        // Build histogram with 40 bins from -3 to 3
        const nBins = 40;
        const range = 3;
        const bins = new Array(nBins).fill(0);
        for (const v of vals) {
          const idx = Math.floor((v + range) / (2 * range) * nBins);
          if (idx >= 0 && idx < nBins) bins[idx]++;
        }
        const maxBin = Math.max(...bins, 1);

        // Draw Gaussian reference curve (standard normal) at high t
        const ab = cosineAlphaBar(t, 1000);
        if (ab < 0.5) {
          histCtx.beginPath();
          histCtx.strokeStyle = 'rgba(236,72,153,0.35)';
          histCtx.lineWidth = 1.5;
          histCtx.setLineDash([4, 3]);
          for (let i = 0; i <= nBins; i++) {
            const x = (i / nBins) * w;
            const v = (i / nBins) * 2 * range - range;
            // Standard Gaussian pdf, scaled to match histogram height
            const pdf = Math.exp(-0.5 * v * v) / Math.sqrt(2 * Math.PI);
            const barH = (pdf * vals.length * (2 * range / nBins) / maxBin) * (h - 20);
            if (i === 0) histCtx.moveTo(x, h - 15 - barH);
            else histCtx.lineTo(x, h - 15 - barH);
          }
          histCtx.stroke();
          histCtx.setLineDash([]);
        }

        // Draw bars
        const barW = w / nBins;
        for (let i = 0; i < nBins; i++) {
          const barH = (bins[i] / maxBin) * (h - 20);
          const x = i * barW;
          const hue = 250 - (i / nBins) * 30;
          histCtx.fillStyle = `hsla(${hue}, 70%, 60%, 0.8)`;
          histCtx.fillRect(x + 1, h - 15 - barH, barW - 2, barH);
        }

        // Draw axis
        histCtx.strokeStyle = '#ccc';
        histCtx.lineWidth = 1;
        histCtx.beginPath();
        histCtx.moveTo(0, h - 15);
        histCtx.lineTo(w, h - 15);
        histCtx.stroke();

        // Labels
        histCtx.fillStyle = '#999';
        histCtx.font = '10px Figtree, sans-serif';
        histCtx.textAlign = 'center';
        histCtx.fillText('-3', 5, h - 2);
        histCtx.fillText('0', w * 0.5, h - 2);
        histCtx.fillText('+3', w - 5, h - 2);
      }

      function render(t) {
        const noisyPx = noiseImage(cleanPixels, noiseVecs, t, 1000);
        renderPixelGrid(ctx, noisyPx, 8, canvas.width);
        drawHistogram(cleanPixels, t);
        tLabel.textContent = `t = ${t} / 1000`;
      }

      slider.addEventListener('input', () => render(parseInt(slider.value)));

      // Shape buttons
      document.querySelectorAll('.nk-shape-btn').forEach(btn => {
        btn.addEventListener('click', () => {
          document.querySelectorAll('.nk-shape-btn').forEach(b => {
            b.classList.remove('btn--primary');
            b.classList.add('btn--secondary');
          });
          btn.classList.remove('btn--secondary');
          btn.classList.add('btn--primary');
          currentShape = btn.dataset.shape;
          cleanPixels = getShapePixels(currentShape);
          render(parseInt(slider.value));
        });
      });

      render(0);
    })();

    // ===== SCORE VECTOR FIELD INTERACTIVE =====
    (function() {
      const canvas = document.getElementById('score-canvas');
      const ctx = canvas.getContext('2d');
      const sampleBtn = document.getElementById('score-sample-btn');
      const resetBtn = document.getElementById('score-reset-btn');

      const W = canvas.width, H = canvas.height;

      // Data clusters (2D Gaussian blobs)
      const clusters = [
        { x: 120, y: 100, sx: 30, sy: 25 },
        { x: 300, y: 200, sx: 35, sy: 30 },
        { x: 200, y: 220, sx: 25, sy: 20 }
      ];

      function gaussian2D(x, y, cx, cy, sx, sy) {
        return Math.exp(-0.5 * (((x-cx)/sx)**2 + ((y-cy)/sy)**2));
      }

      function scoreFn(x, y) {
        let dxTotal = 0, dyTotal = 0, totalW = 0;
        for (const c of clusters) {
          const g = gaussian2D(x, y, c.x, c.y, c.sx, c.sy);
          dxTotal += g * (c.x - x) / (c.sx * c.sx);
          dyTotal += g * (c.y - y) / (c.sy * c.sy);
          totalW += g;
        }
        if (totalW < 1e-10) return { dx: 0, dy: 0 };
        return { dx: dxTotal / totalW, dy: dyTotal / totalW };
      }

      let particles = [];

      function drawField() {
        ctx.clearRect(0, 0, W, H);
        ctx.fillStyle = '#faf9f7';
        ctx.fillRect(0, 0, W, H);

        // Draw density blobs
        for (const c of clusters) {
          const grad = ctx.createRadialGradient(c.x, c.y, 0, c.x, c.y, c.sx * 2.5);
          grad.addColorStop(0, 'rgba(99,102,241,0.3)');
          grad.addColorStop(0.5, 'rgba(139,92,246,0.1)');
          grad.addColorStop(1, 'rgba(139,92,246,0)');
          ctx.fillStyle = grad;
          ctx.beginPath();
          ctx.arc(c.x, c.y, c.sx * 2.5, 0, Math.PI * 2);
          ctx.fill();
        }

        // Draw score arrows
        const step = 30;
        for (let x = step/2; x < W; x += step) {
          for (let y = step/2; y < H; y += step) {
            const { dx, dy } = scoreFn(x, y);
            const mag = Math.sqrt(dx*dx + dy*dy);
            if (mag < 0.001) continue;
            const scale = Math.min(mag * 15, 12);
            const nx = dx / mag * scale;
            const ny = dy / mag * scale;
            const alpha = Math.min(mag * 3, 0.6);

            ctx.strokeStyle = `rgba(99,102,241,${alpha})`;
            ctx.lineWidth = 1;
            ctx.beginPath();
            ctx.moveTo(x, y);
            ctx.lineTo(x + nx, y + ny);
            ctx.stroke();

            // Arrowhead
            const angle = Math.atan2(ny, nx);
            ctx.beginPath();
            ctx.moveTo(x + nx, y + ny);
            ctx.lineTo(x + nx - 4*Math.cos(angle - 0.5), y + ny - 4*Math.sin(angle - 0.5));
            ctx.lineTo(x + nx - 4*Math.cos(angle + 0.5), y + ny - 4*Math.sin(angle + 0.5));
            ctx.fillStyle = `rgba(99,102,241,${alpha})`;
            ctx.fill();
          }
        }

        // Draw particles
        for (const p of particles) {
          // Trail with gradient opacity (fades from old to new)
          if (p.trail.length > 1) {
            for (let i = 1; i < p.trail.length; i++) {
              const alpha = 0.15 + 0.65 * (i / p.trail.length);
              const width = 1 + 2 * (i / p.trail.length);
              ctx.strokeStyle = `rgba(236,72,153,${alpha})`;
              ctx.lineWidth = width;
              ctx.beginPath();
              ctx.moveTo(p.trail[i-1].x, p.trail[i-1].y);
              ctx.lineTo(p.trail[i].x, p.trail[i].y);
              ctx.stroke();
            }
          }

          // Starting point marker (small circle)
          if (p.trail.length > 0) {
            ctx.beginPath();
            ctx.arc(p.trail[0].x, p.trail[0].y, 3, 0, Math.PI * 2);
            ctx.fillStyle = 'rgba(236,72,153,0.25)';
            ctx.fill();
          }

          // Current position (larger, brighter)
          ctx.beginPath();
          ctx.arc(p.x, p.y, 5, 0, Math.PI * 2);
          ctx.fillStyle = '#ec4899';
          ctx.fill();
          ctx.strokeStyle = '#fff';
          ctx.lineWidth = 2;
          ctx.stroke();
        }
      }

      let animId = null;

      function animateParticles() {
        let anyMoving = false;
        for (const p of particles) {
          if (p.steps > 300) continue;
          const { dx, dy } = scoreFn(p.x, p.y);
          const eta = 3;
          const rng = seededRNG(p.steps * 1000 + Math.floor(p.x * 7));
          // Langevin dynamics: follow score + small noise (noise decreases over time)
          const noiseScale = Math.max(0.1, 0.6 * (1 - p.steps / 300));
          p.x += eta * dx + noiseScale * gaussianFromRNG(rng);
          p.y += eta * dy + noiseScale * gaussianFromRNG(rng);
          p.x = Math.max(5, Math.min(W-5, p.x));
          p.y = Math.max(5, Math.min(H-5, p.y));
          p.trail.push({ x: p.x, y: p.y });
          if (p.trail.length > 150) p.trail.shift();
          p.steps++;
          anyMoving = true;
        }
        drawField();
        if (anyMoving) animId = requestAnimationFrame(animateParticles);
      }

      sampleBtn.addEventListener('click', () => {
        const px = Math.random() * W;
        const py = Math.random() * H;
        particles.push({ x: px, y: py, trail: [{ x: px, y: py }], steps: 0 });
        if (!animId) animateParticles();
      });

      resetBtn.addEventListener('click', () => {
        particles = [];
        if (animId) { cancelAnimationFrame(animId); animId = null; }
        drawField();
      });

      drawField();
    })();

    // ===== SCHEDULE COMPARISON INTERACTIVE =====
    (function() {
      const canvas = document.getElementById('sched-canvas');
      const ctx = canvas.getContext('2d');
      let currentSched = 'cosine';

      function linearAlphaBar(t, T) {
        const betaStart = 0.0001, betaEnd = 0.02;
        let ab = 1;
        for (let i = 1; i <= t; i++) {
          const beta = betaStart + (betaEnd - betaStart) * (i / T);
          ab *= (1 - beta);
        }
        return ab;
      }

      function sigmoidAlphaBar(t, T) {
        const x = 12 * (t / T) - 6;
        const sig = 1 / (1 + Math.exp(-x));
        const sig0 = 1 / (1 + Math.exp(6));
        const sig1 = 1 / (1 + Math.exp(-6));
        return 1 - (sig - sig0) / (sig1 - sig0);
      }

      const schedFns = {
        cosine: cosineAlphaBar,
        linear: (t, T) => {
          // Precompute with 200 steps for speed
          const steps = Math.round(t / T * 200);
          return linearAlphaBar(steps, 200);
        },
        sigmoid: sigmoidAlphaBar
      };

      function drawSchedule() {
        const W = canvas.width, H = canvas.height;
        ctx.clearRect(0, 0, W, H);

        const fn = schedFns[currentSched];
        const N = 100;

        // Draw alpha_bar (signal)
        ctx.beginPath();
        ctx.strokeStyle = '#6366f1';
        ctx.lineWidth = 2.5;
        for (let i = 0; i <= N; i++) {
          const t = i;
          const ab = fn(t, N);
          const x = (i / N) * W;
          const y = H - 10 - ab * (H - 20);
          if (i === 0) ctx.moveTo(x, y);
          else ctx.lineTo(x, y);
        }
        ctx.stroke();

        // Draw 1 - alpha_bar (noise)
        ctx.beginPath();
        ctx.strokeStyle = '#ef4444';
        ctx.lineWidth = 2.5;
        for (let i = 0; i <= N; i++) {
          const t = i;
          const ab = fn(t, N);
          const x = (i / N) * W;
          const y = H - 10 - (1 - ab) * (H - 20);
          if (i === 0) ctx.moveTo(x, y);
          else ctx.lineTo(x, y);
        }
        ctx.stroke();

        // Draw SNR (log scale)
        ctx.beginPath();
        ctx.strokeStyle = '#f59e0b';
        ctx.lineWidth = 2;
        ctx.setLineDash([5, 3]);
        for (let i = 0; i <= N; i++) {
          const t = i;
          const ab = fn(t, N);
          const snr = ab / Math.max(1 - ab, 1e-6);
          const logSnr = Math.log10(snr + 1e-6);
          // Map log SNR from -3 to 3 -> 0 to 1
          const norm = (logSnr + 3) / 6;
          const x = (i / N) * W;
          const y = H - 10 - Math.max(0, Math.min(1, norm)) * (H - 20);
          if (i === 0) ctx.moveTo(x, y);
          else ctx.lineTo(x, y);
        }
        ctx.stroke();
        ctx.setLineDash([]);
      }

      document.querySelectorAll('.sched-btn').forEach(btn => {
        btn.addEventListener('click', () => {
          document.querySelectorAll('.sched-btn').forEach(b => { b.classList.remove('btn--primary'); b.classList.add('btn--secondary'); });
          btn.classList.remove('btn--secondary');
          btn.classList.add('btn--primary');
          currentSched = btn.dataset.sched;
          drawSchedule();
        });
      });

      drawSchedule();
    })();

    // ===== SPEED VS QUALITY INTERACTIVE =====
    (function() {
      const gallery = document.getElementById('speed-gallery');
      const slider = document.getElementById('speed-slider');
      const stepsLabel = document.getElementById('speed-steps-label');

      const cleanPx = getShapePixels('smiley');
      const noiseVecs = generateNoiseVecs(64, 321);
      // Extra noise for simulating imperfect denoising
      const extraNoise = generateNoiseVecs(64, 888);

      // Simulate iterative denoising with step-skipping artifacts.
      // With fewer steps, large jumps leave residual noise artifacts.
      function simulateDenoising(numSteps) {
        const totalT = 100;
        const stepSize = totalT / numSteps;
        // Start from pure noise (t=100)
        let currentPx = noiseImage(cleanPx, noiseVecs, totalT, totalT);

        const snapshots = [{ px: currentPx.map(p => [...p]), t: totalT }];

        for (let step = 0; step < numSteps; step++) {
          const tFrom = totalT - step * stepSize;
          const tTo = Math.max(0, totalT - (step + 1) * stepSize);

          // "Ideal" image at tTo
          const idealPx = noiseImage(cleanPx, noiseVecs, tTo, totalT);

          // Simulate imperfect denoising: with large steps, the model's
          // single-step prediction is less accurate. Add residual error
          // proportional to step size.
          const errorScale = Math.min(1, stepSize / 30) * 0.5; // larger steps = more error
          const stepRng = seededRNG(step * 100 + numSteps);
          currentPx = idealPx.map((px, i) => {
            if (tTo === 0 && numSteps >= 20) return px; // clean at the end if enough steps
            const noise = extraNoise[i];
            return [
              Math.max(0, Math.min(255, px[0] + noise[0] * errorScale * 60 * (tTo > 0 ? 1 : 0.5))),
              Math.max(0, Math.min(255, px[1] + noise[1] * errorScale * 60 * (tTo > 0 ? 1 : 0.5))),
              Math.max(0, Math.min(255, px[2] + noise[2] * errorScale * 60 * (tTo > 0 ? 1 : 0.5)))
            ];
          });

          snapshots.push({ px: currentPx.map(p => [...p]), t: Math.round(tTo) });
        }

        return snapshots;
      }

      function renderSpeedGallery(numSteps) {
        gallery.innerHTML = '';
        const snapshots = simulateDenoising(numSteps);

        // Show up to 8 evenly spaced snapshots
        const showIdxs = [];
        if (snapshots.length <= 8) {
          for (let i = 0; i < snapshots.length; i++) showIdxs.push(i);
        } else {
          for (let i = 0; i < 8; i++) {
            showIdxs.push(Math.round(i * (snapshots.length - 1) / 7));
          }
        }

        for (const idx of showIdxs) {
          const { px, t } = snapshots[idx];
          const mini = document.createElement('canvas');
          mini.width = 64; mini.height = 64;
          mini.style.cssText = 'width:64px;height:64px;border-radius:6px;image-rendering:pixelated;border:1px solid #ddd;';
          const miniCtx = mini.getContext('2d');
          renderPixelGrid(miniCtx, px, 8, 64);

          const wrapper = document.createElement('div');
          wrapper.style.cssText = 'text-align:center;';
          wrapper.appendChild(mini);
          const label = document.createElement('div');
          label.style.cssText = 'font-family:Figtree,sans-serif;font-size:0.5rem;color:#999;margin-top:2px;';
          label.textContent = idx === 0 ? 'Start' : idx === snapshots.length - 1 ? `Done (${numSteps} steps)` : `t=${t}`;
          wrapper.appendChild(label);
          gallery.appendChild(wrapper);
        }
      }

      slider.addEventListener('input', () => {
        const steps = parseInt(slider.value);
        stepsLabel.textContent = steps;
        renderSpeedGallery(steps);
      });

      renderSpeedGallery(50);
    })();

    // ===== TRAINING PLAYGROUND INTERACTIVE =====
    (function() {
      const cleanCanvas = document.getElementById('tr-clean');
      const noisyCanvas = document.getElementById('tr-noisy');
      const predCanvas = document.getElementById('tr-predicted');
      const lossCanvas = document.getElementById('tr-loss-chart');
      const playBtn = document.getElementById('tr-play-btn');
      const resetBtn = document.getElementById('tr-reset-btn');
      const timestepLabel = document.getElementById('tr-timestep');
      const epochLabel = document.getElementById('tr-epoch-label');

      const cleanCtx = cleanCanvas.getContext('2d');
      const noisyCtx = noisyCanvas.getContext('2d');
      const predCtx = predCanvas.getContext('2d');
      const lossCtx = lossCanvas.getContext('2d');

      const cleanPx = getShapePixels('star');
      const noiseVecs = generateNoiseVecs(64, 555);

      // Draw clean image
      renderPixelGrid(cleanCtx, cleanPx, 8, cleanCanvas.width);

      let losses = [];
      let epoch = 0;
      let animInterval = null;

      function drawLossChart() {
        const w = lossCanvas.width;
        const h = lossCanvas.height;
        lossCtx.clearRect(0, 0, w, h);
        if (losses.length < 2) return;

        const maxL = Math.max(...losses, 0.01);
        lossCtx.beginPath();
        lossCtx.strokeStyle = '#6366f1';
        lossCtx.lineWidth = 2;
        for (let i = 0; i < losses.length; i++) {
          const x = (i / (losses.length - 1)) * w;
          const y = h - 5 - (losses[i] / maxL) * (h - 10);
          if (i === 0) lossCtx.moveTo(x, y);
          else lossCtx.lineTo(x, y);
        }
        lossCtx.stroke();
      }

      function trainStep() {
        // Random timestep
        const t = Math.floor(Math.random() * 100) + 1;
        timestepLabel.textContent = `t = ${t}`;

        // Noisy image
        const noisyPx = noiseImage(cleanPx, noiseVecs, t, 100);
        renderPixelGrid(noisyCtx, noisyPx, 8, noisyCanvas.width);

        // Simulate "predicted noise" - starts bad, gets better
        const skill = Math.min(epoch / 60, 0.95);
        const predNoise = noiseVecs.map((n, i) => {
          const errRng = seededRNG(epoch * 64 + i);
          return [
            n[0] * skill + gaussianFromRNG(errRng) * (1 - skill),
            n[1] * skill + gaussianFromRNG(errRng) * (1 - skill),
            n[2] * skill + gaussianFromRNG(errRng) * (1 - skill)
          ];
        });

        // Visualize predicted noise as colors
        const predPx = predNoise.map(n => [
          Math.max(0, Math.min(255, 128 + n[0] * 60)),
          Math.max(0, Math.min(255, 128 + n[1] * 60)),
          Math.max(0, Math.min(255, 128 + n[2] * 60))
        ]);
        renderPixelGrid(predCtx, predPx, 8, predCanvas.width);

        // Compute loss
        let mse = 0;
        for (let i = 0; i < noiseVecs.length; i++) {
          for (let c = 0; c < 3; c++) {
            mse += (noiseVecs[i][c] - predNoise[i][c]) ** 2;
          }
        }
        mse /= (noiseVecs.length * 3);
        losses.push(mse);
        if (losses.length > 100) losses = losses.slice(-100);

        epoch++;
        epochLabel.textContent = `Epoch ${epoch}`;
        drawLossChart();
      }

      playBtn.addEventListener('click', () => {
        if (animInterval) {
          clearInterval(animInterval);
          animInterval = null;
          playBtn.textContent = 'Train!';
          return;
        }
        playBtn.textContent = 'Pause';
        animInterval = setInterval(trainStep, 200);
      });

      resetBtn.addEventListener('click', () => {
        if (animInterval) { clearInterval(animInterval); animInterval = null; }
        playBtn.textContent = 'Train!';
        epoch = 0;
        losses = [];
        epochLabel.textContent = 'Epoch 0';
        timestepLabel.textContent = 't = ???';
        lossCtx.clearRect(0, 0, lossCanvas.width, lossCanvas.height);
        noisyCtx.clearRect(0, 0, noisyCanvas.width, noisyCanvas.height);
        predCtx.clearRect(0, 0, predCanvas.width, predCanvas.height);
      });
    })();

    // ===== GENERATOR INTERACTIVE =====
    (function() {
      const canvas = document.getElementById('gen-canvas');
      const ctx = canvas.getContext('2d');
      const filmstrip = document.getElementById('gen-filmstrip');
      const goBtn = document.getElementById('gen-go-btn');
      const seedBtn = document.getElementById('gen-seed-btn');
      const speedSlider = document.getElementById('gen-speed');
      const stepLabel = document.getElementById('gen-step-label');

      // Target shapes to generate
      const targets = ['smiley', 'star', 'heart'];
      let currentSeed = 42;
      let generating = false;

      function getRandomTarget() {
        const rng = seededRNG(currentSeed);
        return targets[Math.floor(rng() * targets.length)];
      }

      async function generate() {
        if (generating) return;
        generating = true;
        goBtn.textContent = 'Generating...';
        goBtn.disabled = true;
        filmstrip.innerHTML = '';

        const target = getRandomTarget();
        const cleanPx = getShapePixels(target);
        const noiseVecs = generateNoiseVecs(64, currentSeed);

        const totalSteps = 100;
        const speed = parseInt(speedSlider.value);
        const delay = Math.max(10, 120 - speed * 12);

        // Start from pure noise
        for (let step = totalSteps; step >= 0; step--) {
          const px = noiseImage(cleanPx, noiseVecs, step, totalSteps);
          renderPixelGrid(ctx, px, 8, canvas.width);
          stepLabel.textContent = `Step ${totalSteps - step} / ${totalSteps}`;

          // Add to filmstrip at intervals
          if (step % 10 === 0 || step === 0) {
            const mini = document.createElement('canvas');
            mini.width = 32; mini.height = 32;
            mini.style.cssText = 'width:32px;height:32px;border-radius:4px;image-rendering:pixelated;border:1px solid #ddd;';
            const miniCtx = mini.getContext('2d');
            renderPixelGrid(miniCtx, px, 8, 32);
            filmstrip.appendChild(mini);
          }

          await new Promise(r => setTimeout(r, delay));
        }

        stepLabel.textContent = 'Done!';
        goBtn.textContent = 'Generate!';
        goBtn.disabled = false;
        generating = false;
      }

      goBtn.addEventListener('click', generate);
      seedBtn.addEventListener('click', () => {
        currentSeed = Math.floor(Math.random() * 100000);
        filmstrip.innerHTML = '';
        stepLabel.textContent = `Seed: ${currentSeed}. Click Generate!`;
        // Show pure noise
        const noiseVecs = generateNoiseVecs(64, currentSeed);
        const noisePx = noiseImage(getShapePixels('smiley'), noiseVecs, 100, 100);
        renderPixelGrid(ctx, noisePx, 8, canvas.width);
      });

      // Show initial noise
      const initNoise = generateNoiseVecs(64, currentSeed);
      const initPx = noiseImage(getShapePixels('smiley'), initNoise, 100, 100);
      renderPixelGrid(ctx, initPx, 8, canvas.width);
    })();

    // ===== REVERSE THE FILM INTERACTIVE =====
    (function() {
      const canvas = document.getElementById('rv-canvas');
      const ctx = canvas.getContext('2d');
      const slider = document.getElementById('rv-slider');
      const tLabel = document.getElementById('rv-t-label');
      const playBtn = document.getElementById('rv-play-btn');
      const resetBtn = document.getElementById('rv-reset-btn');

      const cleanPixels = getShapePixels('heart');
      const noiseVecs = generateNoiseVecs(64, 999);
      let animInterval = null;

      function render(t) {
        const noisyPx = noiseImage(cleanPixels, noiseVecs, t, 100);
        renderPixelGrid(ctx, noisyPx, 8, canvas.width);
        tLabel.textContent = t === 0 ? 't = 0 (clean!)' : t === 100 ? 't = 100 (noise)' : `t = ${t}`;
      }

      slider.addEventListener('input', () => {
        if (animInterval) { clearInterval(animInterval); animInterval = null; playBtn.textContent = 'Auto-reverse'; }
        render(parseInt(slider.value));
      });

      playBtn.addEventListener('click', () => {
        if (animInterval) { clearInterval(animInterval); animInterval = null; playBtn.textContent = 'Auto-reverse'; return; }
        if (parseInt(slider.value) <= 0) slider.value = 100;
        playBtn.textContent = 'Pause';
        animInterval = setInterval(() => {
          let v = parseInt(slider.value) - 1;
          if (v < 0) { clearInterval(animInterval); animInterval = null; playBtn.textContent = 'Auto-reverse'; return; }
          slider.value = v;
          render(v);
        }, 50);
      });

      resetBtn.addEventListener('click', () => {
        if (animInterval) { clearInterval(animInterval); animInterval = null; playBtn.textContent = 'Auto-reverse'; }
        slider.value = 100;
        render(100);
      });

      render(100);
    })();

    // ===== SHORTCUT CALCULATOR INTERACTIVE =====
    (function() {
      const stepCanvas = document.getElementById('sc-stepwise');
      const stepCtx = stepCanvas.getContext('2d');
      const directCanvas = document.getElementById('sc-direct');
      const directCtx = directCanvas.getContext('2d');
      const slider = document.getElementById('sc-slider');
      const tLabel = document.getElementById('sc-t-label');
      const stepsCount = document.getElementById('sc-steps-count');

      const cleanPixels = getShapePixels('smiley');
      // Use the SAME noise vector for both paths
      const noiseVecs = generateNoiseVecs(64, 777);

      function render(t) {
        // Direct jump
        const directPx = noiseImage(cleanPixels, noiseVecs, t, 100);
        renderPixelGrid(directCtx, directPx, 8, directCanvas.width);

        // Stepwise (same result since we use the closed-form)
        renderPixelGrid(stepCtx, directPx, 8, stepCanvas.width);

        tLabel.textContent = `t = ${t}`;
        stepsCount.textContent = t;
      }

      slider.addEventListener('input', () => render(parseInt(slider.value)));
      render(0);
    })();
  </script>

</body>
</html>
