<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>The Chinchilla Paper ‚Äî Training Compute-Optimal Large Language Models</title>
<style>
:root {
  --bg: #fafaf8;
  --text: #1a1a1a;
  --accent: #2563eb;
  --accent-light: #dbeafe;
  --muted: #6b7280;
  --callout-bg: #f8f5f0;
  --callout-border: #d4a574;
  --demo-bg: #ffffff;
  --demo-border: #e5e5e5;
  --green: #16a34a;
  --red: #dc2626;
  --orange: #ea580c;
  --purple: #7c3aed;
}

* { margin: 0; padding: 0; box-sizing: border-box; }

body {
  background: var(--bg);
  color: var(--text);
  font-family: Georgia, 'Times New Roman', serif;
  font-size: 19px;
  line-height: 1.75;
  -webkit-font-smoothing: antialiased;
}

.content {
  max-width: 680px;
  margin: 0 auto;
  padding: 0 24px;
}

/* Hero */
.hero {
  text-align: center;
  padding: 100px 24px 60px;
  max-width: 780px;
  margin: 0 auto;
}
.hero h1 {
  font-family: Georgia, serif;
  font-size: 46px;
  line-height: 1.15;
  font-weight: 700;
  color: var(--text);
  margin-bottom: 16px;
}
.hero .subtitle {
  font-family: system-ui, -apple-system, sans-serif;
  font-size: 20px;
  color: var(--muted);
  margin-bottom: 28px;
  line-height: 1.5;
}
.hero .meta {
  font-family: system-ui, -apple-system, sans-serif;
  font-size: 14px;
  color: var(--muted);
}
.hero .meta span { margin: 0 8px; }

.hero-visual {
  margin: 40px auto 0;
  max-width: 500px;
}

/* Table of Contents */
.toc {
  background: var(--callout-bg);
  border: 1px solid var(--callout-border);
  border-radius: 12px;
  padding: 32px 36px;
  margin: 40px auto 60px;
  max-width: 680px;
}
.toc h2 {
  font-family: system-ui, -apple-system, sans-serif;
  font-size: 14px;
  text-transform: uppercase;
  letter-spacing: 1.5px;
  color: var(--muted);
  margin-bottom: 16px;
}
.toc ol {
  list-style: upper-roman;
  padding-left: 28px;
}
.toc li {
  font-family: system-ui, -apple-system, sans-serif;
  font-size: 16px;
  margin-bottom: 8px;
  line-height: 1.5;
}
.toc a {
  color: var(--text);
  text-decoration: none;
  border-bottom: 1px solid transparent;
  transition: border-color 0.2s;
}
.toc a:hover { border-bottom-color: var(--accent); color: var(--accent); }

/* Sections */
section {
  margin-bottom: 64px;
}
section h2 {
  font-size: 32px;
  font-weight: 700;
  margin-bottom: 20px;
  line-height: 1.25;
  color: var(--text);
}
section h3 {
  font-size: 24px;
  font-weight: 600;
  margin: 32px 0 12px;
  color: var(--text);
}

p {
  margin-bottom: 18px;
}

strong { font-weight: 700; }

a { color: var(--accent); text-decoration: underline; text-decoration-color: var(--accent-light); text-underline-offset: 2px; }
a:hover { text-decoration-color: var(--accent); }

/* Callout */
.callout {
  background: var(--callout-bg);
  border-left: 4px solid var(--callout-border);
  padding: 20px 24px;
  margin: 28px 0;
  border-radius: 0 8px 8px 0;
  font-size: 17px;
}
.callout .callout-title {
  font-family: system-ui, -apple-system, sans-serif;
  font-weight: 700;
  font-size: 14px;
  text-transform: uppercase;
  letter-spacing: 1px;
  color: var(--muted);
  margin-bottom: 8px;
}

/* Demo containers */
.demo-container {
  background: var(--demo-bg);
  border: 1px solid var(--demo-border);
  border-radius: 12px;
  padding: 28px;
  margin: 32px auto;
  max-width: 780px;
  box-shadow: 0 1px 3px rgba(0,0,0,0.04);
}
.demo-container .demo-title {
  font-family: system-ui, -apple-system, sans-serif;
  font-size: 14px;
  font-weight: 700;
  text-transform: uppercase;
  letter-spacing: 1.5px;
  color: var(--accent);
  margin-bottom: 16px;
}
.demo-container .demo-caption {
  font-family: system-ui, -apple-system, sans-serif;
  font-size: 14px;
  color: var(--muted);
  margin-top: 16px;
  line-height: 1.5;
  text-align: center;
}

/* Sliders */
.slider-group {
  margin: 16px 0;
  font-family: system-ui, -apple-system, sans-serif;
}
.slider-group label {
  display: block;
  font-size: 14px;
  font-weight: 600;
  margin-bottom: 6px;
  color: var(--text);
}
.slider-group input[type="range"] {
  width: 100%;
  margin: 4px 0;
  accent-color: var(--accent);
}
.slider-value {
  font-size: 14px;
  color: var(--muted);
  text-align: right;
}

/* Buttons */
.btn {
  font-family: system-ui, -apple-system, sans-serif;
  font-size: 14px;
  font-weight: 600;
  padding: 10px 20px;
  border-radius: 8px;
  border: none;
  cursor: pointer;
  transition: all 0.2s;
  background: var(--accent);
  color: white;
}
.btn:hover { background: #1d4ed8; transform: translateY(-1px); }
.btn-outline {
  background: transparent;
  color: var(--accent);
  border: 2px solid var(--accent);
}
.btn-outline:hover { background: var(--accent-light); }

/* Bar Chart */
.bar-chart {
  display: flex;
  align-items: flex-end;
  gap: 8px;
  height: 220px;
  padding: 0 8px;
  margin: 20px 0;
}
.bar-wrapper {
  display: flex;
  flex-direction: column;
  align-items: center;
  flex: 1;
  height: 100%;
  justify-content: flex-end;
}
.bar {
  width: 100%;
  border-radius: 6px 6px 0 0;
  transition: height 0.6s cubic-bezier(0.4,0,0.2,1), background 0.3s;
  min-height: 4px;
  position: relative;
}
.bar-label {
  font-family: system-ui, -apple-system, sans-serif;
  font-size: 11px;
  color: var(--muted);
  margin-top: 8px;
  text-align: center;
  line-height: 1.3;
}
.bar-value {
  font-family: system-ui, -apple-system, sans-serif;
  font-size: 12px;
  font-weight: 700;
  color: var(--text);
  margin-bottom: 4px;
}

/* Step-through */
.step-nav {
  display: flex;
  justify-content: center;
  gap: 12px;
  margin-top: 20px;
}
.step-indicator {
  display: flex;
  justify-content: center;
  gap: 6px;
  margin-top: 12px;
}
.step-dot {
  width: 10px; height: 10px;
  border-radius: 50%;
  background: var(--demo-border);
  transition: background 0.3s;
}
.step-dot.active { background: var(--accent); }

/* Comparison cards */
.model-cards {
  display: grid;
  grid-template-columns: 1fr 1fr;
  gap: 16px;
  margin: 20px 0;
}
.model-card {
  border: 2px solid var(--demo-border);
  border-radius: 10px;
  padding: 20px;
  text-align: center;
  transition: all 0.3s;
  cursor: pointer;
}
.model-card:hover { border-color: var(--accent); transform: translateY(-2px); box-shadow: 0 4px 12px rgba(37,99,235,0.1); }
.model-card.winner { border-color: var(--green); background: #f0fdf4; }
.model-card .model-name {
  font-family: system-ui, -apple-system, sans-serif;
  font-size: 22px;
  font-weight: 800;
  margin-bottom: 4px;
}
.model-card .model-detail {
  font-family: system-ui, -apple-system, sans-serif;
  font-size: 13px;
  color: var(--muted);
  margin-bottom: 12px;
}
.model-card .model-stat {
  font-family: system-ui, -apple-system, sans-serif;
  font-size: 28px;
  font-weight: 800;
}

/* Quiz */
.quiz-options {
  display: flex;
  flex-direction: column;
  gap: 10px;
  margin: 16px 0;
}
.quiz-option {
  font-family: system-ui, -apple-system, sans-serif;
  font-size: 15px;
  padding: 14px 18px;
  border: 2px solid var(--demo-border);
  border-radius: 8px;
  background: white;
  cursor: pointer;
  transition: all 0.2s;
  text-align: left;
}
.quiz-option:hover { border-color: var(--accent); background: var(--accent-light); }
.quiz-option.correct { border-color: var(--green); background: #f0fdf4; }
.quiz-option.wrong { border-color: var(--red); background: #fef2f2; }
.quiz-feedback {
  font-family: system-ui, -apple-system, sans-serif;
  font-size: 15px;
  padding: 12px 16px;
  border-radius: 8px;
  margin-top: 12px;
  display: none;
}

/* Inline number highlight */
.num {
  font-family: system-ui, -apple-system, sans-serif;
  font-weight: 700;
  color: var(--accent);
}

/* Grid layout */
.flex-row { display: flex; gap: 16px; align-items: center; flex-wrap: wrap; }

/* SVG styles */
svg text {
  font-family: system-ui, -apple-system, sans-serif;
}

/* Responsive */
@media (max-width: 600px) {
  .hero h1 { font-size: 32px; }
  .hero { padding: 60px 20px 40px; }
  .model-cards { grid-template-columns: 1fr; }
  .demo-container { padding: 20px 16px; }
  section h2 { font-size: 26px; }
  body { font-size: 17px; }
}

/* Footer */
footer {
  text-align: center;
  padding: 60px 24px 80px;
  font-family: system-ui, -apple-system, sans-serif;
  font-size: 14px;
  color: var(--muted);
}
footer a { color: var(--muted); }

/* Tooltip */
.tooltip {
  position: relative;
  display: inline-block;
  border-bottom: 1px dashed var(--muted);
  cursor: help;
}
.tooltip .tooltip-text {
  visibility: hidden;
  opacity: 0;
  background: var(--text);
  color: white;
  font-family: system-ui, -apple-system, sans-serif;
  font-size: 13px;
  line-height: 1.4;
  padding: 8px 12px;
  border-radius: 6px;
  position: absolute;
  z-index: 10;
  bottom: 130%;
  left: 50%;
  transform: translateX(-50%);
  width: 220px;
  text-align: center;
  transition: opacity 0.2s;
}
.tooltip:hover .tooltip-text { visibility: visible; opacity: 1; }

/* Animated highlight */
@keyframes highlightPulse {
  0%, 100% { box-shadow: 0 0 0 0 rgba(37,99,235,0.3); }
  50% { box-shadow: 0 0 0 8px rgba(37,99,235,0); }
}
.pulse { animation: highlightPulse 2s infinite; }

/* Progress bars */
.progress-bar-track {
  width: 100%;
  height: 24px;
  background: #f0f0f0;
  border-radius: 12px;
  overflow: hidden;
  margin: 8px 0;
}
.progress-bar-fill {
  height: 100%;
  border-radius: 12px;
  transition: width 0.8s cubic-bezier(0.4,0,0.2,1);
  display: flex;
  align-items: center;
  padding-left: 10px;
  font-family: system-ui, sans-serif;
  font-size: 12px;
  font-weight: 700;
  color: white;
}

/* Timeline */
.timeline {
  position: relative;
  padding-left: 32px;
  margin: 24px 0;
}
.timeline::before {
  content: '';
  position: absolute;
  left: 8px;
  top: 0;
  bottom: 0;
  width: 3px;
  background: var(--demo-border);
  border-radius: 2px;
}
.timeline-item {
  position: relative;
  margin-bottom: 24px;
  cursor: pointer;
  transition: all 0.3s;
}
.timeline-item::before {
  content: '';
  position: absolute;
  left: -28px;
  top: 6px;
  width: 14px;
  height: 14px;
  border-radius: 50%;
  background: var(--demo-border);
  border: 3px solid var(--bg);
  transition: background 0.3s;
}
.timeline-item:hover::before, .timeline-item.active::before {
  background: var(--accent);
}
.timeline-item .tl-year {
  font-family: system-ui, sans-serif;
  font-size: 13px;
  font-weight: 700;
  color: var(--accent);
}
.timeline-item .tl-title {
  font-family: system-ui, sans-serif;
  font-size: 16px;
  font-weight: 600;
}
.timeline-item .tl-detail {
  font-family: system-ui, sans-serif;
  font-size: 14px;
  color: var(--muted);
  max-height: 0;
  overflow: hidden;
  transition: max-height 0.4s ease;
}
.timeline-item.active .tl-detail { max-height: 200px; }

/* Canvas wrapper */
.canvas-wrap {
  width: 100%;
  display: flex;
  justify-content: center;
  margin: 12px 0;
}
.canvas-wrap canvas {
  max-width: 100%;
}

/* Data table */
.data-table {
  width: 100%;
  border-collapse: collapse;
  font-family: system-ui, sans-serif;
  font-size: 14px;
  margin: 16px 0;
}
.data-table th, .data-table td {
  padding: 10px 14px;
  text-align: center;
  border-bottom: 1px solid var(--demo-border);
}
.data-table th {
  font-weight: 700;
  font-size: 12px;
  text-transform: uppercase;
  letter-spacing: 0.5px;
  color: var(--muted);
  background: #f9f9f7;
}
.data-table tr:hover { background: var(--accent-light); }
.data-table .highlight-row { background: #f0fdf4; font-weight: 700; }

/* Input group */
.input-group {
  font-family: system-ui, sans-serif;
  display: flex;
  gap: 12px;
  align-items: center;
  margin: 12px 0;
  flex-wrap: wrap;
}
.input-group input[type="number"],
.input-group input[type="text"] {
  font-size: 15px;
  padding: 8px 12px;
  border: 2px solid var(--demo-border);
  border-radius: 8px;
  width: 140px;
  font-family: system-ui, sans-serif;
}
.input-group input:focus { outline: none; border-color: var(--accent); }
.result-display {
  font-family: system-ui, sans-serif;
  font-size: 16px;
  font-weight: 700;
  color: var(--accent);
  padding: 12px 16px;
  background: var(--accent-light);
  border-radius: 8px;
  text-align: center;
  margin: 12px 0;
  min-height: 48px;
}

/* Scroll animate */
.fade-in {
  opacity: 0;
  transform: translateY(24px);
  transition: opacity 0.6s ease, transform 0.6s ease;
}
.fade-in.visible {
  opacity: 1;
  transform: translateY(0);
}

/* Equation box */
.equation-box {
  background: #f0f0ee;
  border-radius: 10px;
  padding: 20px 24px;
  margin: 24px 0;
  text-align: center;
  font-family: Georgia, serif;
  font-size: 20px;
  font-style: italic;
  line-height: 1.6;
  letter-spacing: 0.3px;
}
.equation-box .eq-label {
  font-family: system-ui, sans-serif;
  font-size: 12px;
  font-style: normal;
  font-weight: 600;
  text-transform: uppercase;
  letter-spacing: 1px;
  color: var(--muted);
  display: block;
  margin-bottom: 8px;
}

/* Hover card */
.hover-grid {
  display: grid;
  grid-template-columns: repeat(auto-fill, minmax(140px, 1fr));
  gap: 12px;
  margin: 16px 0;
}
.hover-card {
  background: white;
  border: 2px solid var(--demo-border);
  border-radius: 10px;
  padding: 16px;
  text-align: center;
  cursor: pointer;
  transition: all 0.3s;
  font-family: system-ui, sans-serif;
}
.hover-card:hover {
  border-color: var(--accent);
  transform: translateY(-3px);
  box-shadow: 0 6px 16px rgba(37,99,235,0.12);
}
.hover-card .hc-value {
  font-size: 22px;
  font-weight: 800;
  color: var(--accent);
}
.hover-card .hc-label {
  font-size: 12px;
  color: var(--muted);
  margin-top: 4px;
}
.hover-card .hc-detail {
  font-size: 12px;
  color: var(--text);
  margin-top: 8px;
  max-height: 0;
  overflow: hidden;
  transition: max-height 0.3s;
}
.hover-card:hover .hc-detail { max-height: 80px; }
</style>
</head>
<body>

<!-- ============== HERO ============== -->
<div class="hero">
  <h1>The Chinchilla Paper</h1>
  <p class="subtitle">Training Compute-Optimal Large Language Models ‚Äî and why throwing more parameters at the problem was the wrong strategy all along.</p>
  <p class="meta">Hoffmann et al., 2022 &nbsp;¬∑&nbsp; ~25 min read &nbsp;¬∑&nbsp; Interactive explainer</p>
  <div class="hero-visual">
    <svg viewBox="0 0 500 200" xmlns="http://www.w3.org/2000/svg" id="heroSvg">
      <defs>
        <linearGradient id="heroGrad" x1="0" y1="0" x2="1" y2="0">
          <stop offset="0%" stop-color="#2563eb" stop-opacity="0.15"/>
          <stop offset="100%" stop-color="#7c3aed" stop-opacity="0.15"/>
        </linearGradient>
      </defs>
      <rect x="0" y="0" width="500" height="200" rx="16" fill="url(#heroGrad)"/>
      <!-- Gopher (big, sad) -->
      <circle cx="150" cy="110" r="55" fill="#e5e7eb" stroke="#d1d5db" stroke-width="2"/>
      <text x="150" y="105" text-anchor="middle" font-size="14" font-weight="700" fill="#6b7280">Gopher</text>
      <text x="150" y="122" text-anchor="middle" font-size="11" fill="#9ca3af">280B params</text>
      <text x="150" y="138" text-anchor="middle" font-size="10" fill="#9ca3af">300B tokens</text>
      <!-- Chinchilla (small, happy) -->
      <circle cx="350" cy="110" r="38" fill="#dbeafe" stroke="#2563eb" stroke-width="2.5" id="chinchCircle"/>
      <text x="350" y="107" text-anchor="middle" font-size="14" font-weight="700" fill="#2563eb">Chinchilla</text>
      <text x="350" y="122" text-anchor="middle" font-size="11" fill="#3b82f6">70B params</text>
      <text x="350" y="136" text-anchor="middle" font-size="10" fill="#3b82f6">1.4T tokens</text>
      <!-- Arrow -->
      <text x="250" y="90" text-anchor="middle" font-size="24" fill="#16a34a">‚Üí</text>
      <text x="250" y="150" text-anchor="middle" font-size="12" font-weight="600" fill="#16a34a">Wins!</text>
      <!-- Crown on chinchilla -->
      <text x="350" y="65" text-anchor="middle" font-size="22">üëë</text>
    </svg>
  </div>
</div>

<!-- ============== TABLE OF CONTENTS ============== -->
<div class="content">
  <div class="toc">
    <h2>Table of Contents</h2>
    <ol>
      <li><a href="#sec-hook">The Billion-Dollar Mistake</a></li>
      <li><a href="#sec-context">Before Chinchilla: The Scaling Gold Rush</a></li>
      <li><a href="#sec-core">The Core Insight: Equal Scaling</a></li>
      <li><a href="#sec-approaches">Three Approaches, One Answer</a></li>
      <li><a href="#sec-showdown">The Showdown: Chinchilla vs. Gopher</a></li>
      <li><a href="#sec-math">The Math Behind Compute-Optimal Scaling</a></li>
      <li><a href="#sec-kaplan">Kaplan vs. Hoffmann: Dueling Scaling Laws</a></li>
      <li><a href="#sec-tax">The Chinchilla Tax</a></li>
      <li><a href="#sec-industry">How This Changed Every AI Lab</a></li>
      <li><a href="#sec-limits">Limitations and the Data Wall</a></li>
      <li><a href="#sec-legacy">Legacy and What Came Next</a></li>
      <li><a href="#sec-summary">Summary: The Scaling Cheat Sheet</a></li>
    </ol>
  </div>
</div>

<!-- ============== SECTION I ============== -->
<div class="content">
  <section id="sec-hook" class="fade-in">
    <h2>I. The Billion-Dollar Mistake</h2>

    <p>In 2020 and 2021, the AI world had a simple mantra: <strong>bigger is better</strong>. GPT-3 had 175 billion parameters. Google's PaLM would push to 540 billion. Labs were racing to train the largest model they could afford, often spending tens of millions of dollars on a single training run.</p>

    <p>There was just one problem. <strong>They were all doing it wrong.</strong></p>

    <p>In March 2022, a team at DeepMind published a paper that sent shockwaves through the AI industry. Their finding was as simple as it was devastating: virtually every large language model in existence had been <strong>significantly undertrained</strong>. These models had too many parameters and were trained on far too little data.</p>

    <p>The proof? A model called <strong>Chinchilla</strong> ‚Äî with just 70 billion parameters, less than half the size of DeepMind's own Gopher (280B) ‚Äî outperformed Gopher on nearly every benchmark. It used the <em>exact same compute budget</em>. The only difference was how that compute was allocated: fewer parameters, more data.</p>

    <p>This wasn't a marginal improvement. It was a paradigm shift. Let's start with a question to see what your intuition says.</p>

    <!-- Demo 1: Prediction challenge -->
    <div class="demo-container">
      <div class="demo-title">üéØ Interactive ‚Äî Prediction Challenge</div>
      <p style="font-family: system-ui, sans-serif; font-size: 15px; margin-bottom: 16px;">You have a fixed compute budget of <strong>5.76 √ó 10¬≤¬≥ FLOPs</strong>. How would <em>you</em> allocate it? Move the slider to choose your split between model size and training data.</p>

      <div class="slider-group">
        <label>Model Size: <span id="predParams">175B</span> parameters</label>
        <input type="range" min="1" max="500" value="175" id="predSlider" oninput="updatePrediction()">
        <div class="slider-value">Training Data: <span id="predTokens">550B</span> tokens</div>
      </div>

      <div id="predResult" class="result-display" style="margin-top:16px;">
        Move the slider to see the estimated loss
      </div>

      <div class="demo-caption">The compute-optimal point is <em>not</em> where most labs put it in 2021. Can you find the sweet spot?</div>
    </div>
  </section>

  <!-- ============== SECTION II ============== -->
  <section id="sec-context" class="fade-in">
    <h2>II. Before Chinchilla: The Scaling Gold Rush</h2>

    <p>To understand why the Chinchilla paper mattered, we need to rewind. The year is 2020. OpenAI has just released <strong>GPT-3</strong>, and the world is stunned. The model can write poetry, code, and even do basic math ‚Äî all from a single prompt. The message was clear: scale up the parameters, and intelligence follows.</p>

    <p>This belief wasn't unfounded. In January 2020, <strong>Kaplan et al.</strong> at OpenAI published "Scaling Laws for Neural Language Models" ‚Äî a landmark paper that showed <strong>smooth, predictable improvements</strong> in model performance as you increased three things: parameters (N), data (D), and compute (C).</p>

    <p>But Kaplan's paper had a particular conclusion that shaped the entire industry's strategy: it suggested that model performance was <strong>more sensitive to the number of parameters than to the amount of training data</strong>. The implication? If you had a bigger compute budget, you should mostly spend it on a bigger model.</p>

    <p>And so began the great parameter arms race.</p>

    <!-- Demo 2: Timeline -->
    <div class="demo-container">
      <div class="demo-title">üìÖ Interactive ‚Äî The Parameter Arms Race</div>
      <p style="font-family: system-ui, sans-serif; font-size: 15px; margin-bottom: 16px;">Click on each milestone to see how the race unfolded. Notice how parameters grew much faster than training data.</p>

      <div class="timeline" id="timeline">
        <div class="timeline-item" onclick="toggleTimeline(this)">
          <div class="tl-year">Feb 2019</div>
          <div class="tl-title">GPT-2 ‚Äî 1.5B parameters</div>
          <div class="tl-detail">Trained on ~40B tokens (WebText). OpenAI famously withheld release, fearing misuse. The data-to-params ratio: ~27 tokens per parameter.</div>
        </div>
        <div class="timeline-item" onclick="toggleTimeline(this)">
          <div class="tl-year">Jan 2020</div>
          <div class="tl-title">Kaplan Scaling Laws published</div>
          <div class="tl-detail">Suggested scaling parameters faster than data. This single paper influenced compute allocation at every major lab for the next two years.</div>
        </div>
        <div class="timeline-item" onclick="toggleTimeline(this)">
          <div class="tl-year">Jun 2020</div>
          <div class="tl-title">GPT-3 ‚Äî 175B parameters</div>
          <div class="tl-detail">Trained on ~300B tokens. Ratio: ~1.7 tokens per parameter. A 100√ó jump in parameters, but only ~7.5√ó more data. The pattern was set.</div>
        </div>
        <div class="timeline-item" onclick="toggleTimeline(this)">
          <div class="tl-year">Jan 2022</div>
          <div class="tl-title">Megatron-Turing NLG ‚Äî 530B parameters</div>
          <div class="tl-detail">NVIDIA + Microsoft's entry. Trained on ~339B tokens. Ratio: ~0.6 tokens per parameter. Even more lopsided toward parameters.</div>
        </div>
        <div class="timeline-item" onclick="toggleTimeline(this)">
          <div class="tl-year">Dec 2021</div>
          <div class="tl-title">Gopher ‚Äî 280B parameters</div>
          <div class="tl-detail">DeepMind's flagship. Trained on 300B tokens. Ratio: ~1.1 tokens per parameter. Strong performance, but was it optimal?</div>
        </div>
        <div class="timeline-item active" onclick="toggleTimeline(this)">
          <div class="tl-year">Mar 2022</div>
          <div class="tl-title" style="color: var(--accent);">Chinchilla ‚Äî 70B parameters ‚ú®</div>
          <div class="tl-detail" style="max-height:200px">Trained on 1.4 TRILLION tokens. Ratio: ~20 tokens per parameter. Used the same compute as Gopher but allocated it radically differently. Outperformed everything.</div>
        </div>
      </div>

      <div class="demo-caption">Notice the tokens-per-parameter ratio. Pre-Chinchilla models were heavily skewed toward parameters. Chinchilla inverted the strategy.</div>
    </div>

    <p>The prevailing wisdom was essentially: "We have X FLOPs. Let's make the model as big as possible, train it for one epoch on whatever data we have, and call it done." The Chinchilla paper showed this was leaving <strong>enormous performance on the table</strong>.</p>
  </section>

  <!-- ============== SECTION III ============== -->
  <section id="sec-core" class="fade-in">
    <h2>III. The Core Insight: Equal Scaling</h2>

    <p>Here is the central claim of the Chinchilla paper, stated as plainly as possible:</p>

    <div class="callout">
      <div class="callout-title">Key Finding</div>
      For compute-optimal training, the number of <strong>model parameters</strong> and the number of <strong>training tokens</strong> should be scaled <strong>equally</strong> ‚Äî in roughly equal proportions ‚Äî as compute budget increases.
    </div>

    <p>Let's unpack what this means. If you double your compute budget, you shouldn't just double the model size. You should increase <em>both</em> the model size <em>and</em> the training data by roughly the same factor ‚Äî about <strong>1.4√ó each</strong> (since 1.4 √ó 1.4 ‚âà 2).</p>

    <p>The paper proposes that the optimal number of training tokens should be approximately <strong>20√ó the number of parameters</strong>. A 10B parameter model should see ~200B tokens. A 70B model should see ~1.4T tokens. A 500B model would need ~10T tokens.</p>

    <p>This was a radical departure from Kaplan's earlier findings, which suggested parameters should grow faster than data. Let's visualize what this means in practice.</p>

    <!-- Demo 3: Equal Scaling Visualizer -->
    <div class="demo-container">
      <div class="demo-title">‚öñÔ∏è Interactive ‚Äî The Equal Scaling Rule</div>
      <p style="font-family: system-ui, sans-serif; font-size: 15px; margin-bottom: 16px;">Use the slider to increase compute budget and watch how the optimal allocation of parameters and data should grow together.</p>

      <div class="slider-group">
        <label>Compute Budget: <span id="computeLabel">10¬≤¬π</span> FLOPs</label>
        <input type="range" min="19" max="26" step="0.5" value="21" id="computeSlider" oninput="updateEqualScale()">
      </div>

      <div style="display:flex; gap:24px; margin-top:16px;">
        <div style="flex:1;">
          <div style="font-family:system-ui,sans-serif;font-size:13px;font-weight:600;color:var(--muted);margin-bottom:8px;">Optimal Parameters</div>
          <div class="progress-bar-track">
            <div class="progress-bar-fill" id="paramBar" style="width:20%; background:var(--accent);">
              <span id="paramBarLabel">400M</span>
            </div>
          </div>
        </div>
        <div style="flex:1;">
          <div style="font-family:system-ui,sans-serif;font-size:13px;font-weight:600;color:var(--muted);margin-bottom:8px;">Optimal Tokens</div>
          <div class="progress-bar-track">
            <div class="progress-bar-fill" id="tokenBar" style="width:20%; background:var(--purple);">
              <span id="tokenBarLabel">8.0B</span>
            </div>
          </div>
        </div>
      </div>

      <div class="result-display" id="ratioDisplay" style="margin-top:16px;">
        Tokens-per-parameter ratio: ~20√ó
      </div>

      <div class="demo-caption">Both bars should grow at roughly the same rate. The optimal tokens-per-parameter ratio stays near <strong>20√ó</strong> regardless of scale.</div>
    </div>

    <p>Think of it like cooking. Kaplan's earlier work was like saying: "If you want a better meal, just get a bigger oven." The Chinchilla paper says: "Actually, a slightly smaller oven <em>with more ingredients</em> makes a much better dish." The oven is your model; the ingredients are your data.</p>
  </section>

  <!-- ============== SECTION IV ============== -->
  <section id="sec-approaches" class="fade-in">
    <h2>IV. Three Approaches, One Answer</h2>

    <p>One of the most compelling aspects of the Chinchilla paper is that the authors didn't rely on a single methodology. They used <strong>three completely independent approaches</strong> to derive the optimal scaling law ‚Äî and all three converged on the same answer. That's the kind of evidence that makes a paper hard to argue with.</p>

    <!-- Demo 4: Step-through approaches -->
    <div class="demo-container">
      <div class="demo-title">üî¨ Interactive ‚Äî Three Paths to One Truth</div>
      <p style="font-family: system-ui, sans-serif; font-size: 15px; margin-bottom: 12px;">Step through each approach to see how they derived the optimal scaling.</p>

      <div id="approachContent" style="min-height: 240px;">
        <div id="approach1" class="approach-panel">
          <h3 style="font-family:system-ui,sans-serif; font-size:18px; color:var(--accent); margin-bottom:12px;">Approach 1: Fix Compute, Vary Allocation</h3>
          <p style="font-family:system-ui,sans-serif; font-size:15px; line-height:1.6;">For each of several fixed compute budgets (from 10¬π‚Å∏ to 10¬≤¬π FLOPs), they trained <strong>over 400 models</strong> ranging from 70M to 16B parameters.</p>
          <p style="font-family:system-ui,sans-serif; font-size:15px; line-height:1.6; margin-top:8px;">For each budget level, they varied how much was allocated to model size vs. training data. Then they found which allocation produced the <strong>lowest loss</strong> ‚Äî the "valley" of the loss curve.</p>
          <svg viewBox="0 0 400 180" style="margin:12px auto; display:block;">
            <text x="200" y="16" text-anchor="middle" font-size="12" fill="#6b7280" font-weight="600">Loss vs. Model Size (for fixed compute)</text>
            <!-- Axes -->
            <line x1="50" y1="150" x2="370" y2="150" stroke="#d1d5db" stroke-width="1.5"/>
            <line x1="50" y1="150" x2="50" y2="30" stroke="#d1d5db" stroke-width="1.5"/>
            <text x="210" y="172" text-anchor="middle" font-size="11" fill="#6b7280">Model Parameters ‚Üí</text>
            <text x="18" y="90" text-anchor="middle" font-size="11" fill="#6b7280" transform="rotate(-90,18,90)">Loss ‚Üí</text>
            <!-- U-shaped curve -->
            <path d="M70,55 Q130,140 200,75 Q270,30 360,100" fill="none" stroke="#2563eb" stroke-width="2.5"/>
            <!-- Optimal point -->
            <circle cx="240" cy="42" r="6" fill="#16a34a" stroke="white" stroke-width="2"/>
            <text x="240" y="32" text-anchor="middle" font-size="10" fill="#16a34a" font-weight="700">Optimal</text>
            <!-- Labels -->
            <text x="90" y="48" font-size="10" fill="#9ca3af">Too small</text>
            <text x="320" y="92" font-size="10" fill="#9ca3af">Too big</text>
          </svg>
          <p style="font-family:system-ui,sans-serif; font-size:14px; color:var(--muted); text-align:center;">Each compute level produces a U-shaped curve. Too few parameters wastes compute on unnecessary passes; too many leaves each parameter undertrained.</p>
        </div>

        <div id="approach2" class="approach-panel" style="display:none;">
          <h3 style="font-family:system-ui,sans-serif; font-size:18px; color:var(--purple); margin-bottom:12px;">Approach 2: Parametric Loss Function (IsoFLOP)</h3>
          <p style="font-family:system-ui,sans-serif; font-size:15px; line-height:1.6;">They fitted a <strong>parametric model</strong> of the loss function as a function of both N (parameters) and D (data):</p>
          <div class="equation-box" style="font-size:17px;">
            <span class="eq-label">Parametric Loss Model</span>
            L(N, D) = E + A / N<sup>Œ±</sup> + B / D<sup>Œ≤</sup>
          </div>
          <p style="font-family:system-ui,sans-serif; font-size:15px; line-height:1.6; margin-top:8px;">Where <strong>E</strong> is the irreducible loss (entropy of natural language), and the other terms capture how loss decreases with more parameters and data. By fitting Œ± and Œ≤ from training runs, they could <strong>predict the optimal allocation</strong> for any compute budget.</p>
          <p style="font-family:system-ui,sans-serif; font-size:14px; color:var(--muted); margin-top:8px;">Result: Œ± ‚âà 0.34, Œ≤ ‚âà 0.28 ‚Äî remarkably similar exponents, confirming that parameters and data matter roughly equally.</p>
        </div>

        <div id="approach3" class="approach-panel" style="display:none;">
          <h3 style="font-family:system-ui,sans-serif; font-size:18px; color:var(--orange); margin-bottom:12px;">Approach 3: Direct Fit of Optimal N and D</h3>
          <p style="font-family:system-ui,sans-serif; font-size:15px; line-height:1.6;">The simplest approach: take all the optimal (N, D) pairs found from Approach 1, plot them against compute, and fit a power law directly.</p>
          <p style="font-family:system-ui,sans-serif; font-size:15px; line-height:1.6; margin-top:8px;">They found:</p>
          <div class="equation-box" style="font-size:17px;">
            <span class="eq-label">Optimal Scaling</span>
            N<sub>opt</sub> ‚àù C<sup>0.50</sup> &nbsp;&nbsp;and&nbsp;&nbsp; D<sub>opt</sub> ‚àù C<sup>0.50</sup>
          </div>
          <p style="font-family:system-ui,sans-serif; font-size:15px; line-height:1.6; margin-top:8px;">Both scale as the <strong>square root of compute</strong> ‚Äî meaning they grow at exactly the same rate. Double compute ‚Üí ~1.41√ó more parameters AND ~1.41√ó more data.</p>
          <p style="font-family:system-ui,sans-serif; font-size:14px; color:var(--green); font-weight:600; margin-top:12px;">‚úì All three approaches agree: scale parameters and data equally.</p>
        </div>
      </div>

      <div class="step-nav">
        <button class="btn btn-outline" onclick="showApproach(-1)">‚Üê Previous</button>
        <button class="btn" onclick="showApproach(1)">Next ‚Üí</button>
      </div>
      <div class="step-indicator" id="approachDots">
        <div class="step-dot active"></div>
        <div class="step-dot"></div>
        <div class="step-dot"></div>
      </div>

      <div class="demo-caption">Three independent methodologies. One consistent answer. That's how you move an entire industry.</div>
    </div>

    <p>The convergence of all three approaches is what gave the paper its weight. Any one method might have had flaws or biases, but when three fundamentally different analytical strategies agree, you can be pretty confident in the result.</p>
  </section>

  <!-- ============== SECTION V ============== -->
  <section id="sec-showdown" class="fade-in">
    <h2>V. The Showdown: Chinchilla vs. Gopher</h2>

    <p>Theory is nice. But nothing convinces like a head-to-head battle. The Chinchilla paper didn't just propose a new scaling law ‚Äî it <strong>built a model to prove it</strong>.</p>

    <p><strong>Gopher</strong> was DeepMind's state-of-the-art LLM: 280 billion parameters, trained on 300 billion tokens, using approximately 5.76 √ó 10¬≤¬≥ FLOPs of compute.</p>

    <p><strong>Chinchilla</strong> used the exact same compute budget. But instead of 280B parameters, it had just 70B ‚Äî a quarter the size. The saved compute was redirected into training on 4.7√ó more data: 1.4 trillion tokens.</p>

    <p>The result? Chinchilla <em>uniformly outperformed Gopher</em>. Not on one benchmark. On virtually all of them.</p>

    <!-- Demo 5: Benchmark Comparison -->
    <div class="demo-container">
      <div class="demo-title">üèÜ Interactive ‚Äî Benchmark Battle</div>
      <p style="font-family: system-ui, sans-serif; font-size: 15px; margin-bottom: 16px;">Click each benchmark to see how Chinchilla (70B) compared to Gopher (280B). <span style="color:var(--green);font-weight:600;">Green</span> = Chinchilla wins.</p>

      <div id="benchmarkBars" style="margin: 20px 0;">
        <!-- Rendered by JS -->
      </div>

      <div id="benchmarkDetail" class="result-display" style="font-size:14px;">
        Click a benchmark above to see details
      </div>

      <div class="demo-caption">Same compute budget. 4√ó fewer parameters. Better results on every task. That's the power of compute-optimal training.</div>
    </div>

    <p>This wasn't just a theoretical exercise. It had immediate practical implications: <strong>a model 4√ó smaller is 4√ó cheaper to serve</strong>. Inference costs ‚Äî the cost of actually running the model for users ‚Äî scale roughly linearly with parameter count. Chinchilla delivered better performance <em>and</em> cost less to deploy.</p>

    <div class="callout">
      <div class="callout-title">Why This Matters for Deployment</div>
      A 70B model requires roughly <strong>140 GB of memory</strong> in fp16, while 280B requires ~560 GB. That's the difference between running on a single server vs. needing a multi-node setup. Chinchilla wasn't just smarter ‚Äî it was dramatically more practical.
    </div>
  </section>

  <!-- ============== SECTION VI ============== -->
  <section id="sec-math" class="fade-in">
    <h2>VI. The Math Behind Compute-Optimal Scaling</h2>

    <p>Let's get into the equations ‚Äî but gently. The core math is surprisingly elegant.</p>

    <p>The total compute <strong>C</strong> for training a transformer is approximately:</p>

    <div class="equation-box">
      <span class="eq-label">Compute Approximation</span>
      C ‚âà 6 √ó N √ó D
    </div>

    <p>Where <strong>N</strong> is the number of parameters and <strong>D</strong> is the number of training tokens. The factor of 6 comes from the forward and backward passes through the network (roughly 2 FLOPs per parameter per token for the forward pass, and 4 for backward).</p>

    <p>The Chinchilla paper's key insight is that the loss function has a specific shape. If you fix compute C, there's a <strong>unique optimal pair (N*, D*)</strong> that minimizes loss. And that pair follows:</p>

    <div class="equation-box">
      <span class="eq-label">Chinchilla Optimal Scaling</span>
      N<sub>opt</sub> ‚âà 0.0624 √ó C<sup>0.50</sup><br>
      D<sub>opt</sub> ‚âà 0.2167 √ó C<sup>0.50</sup>
    </div>

    <p>Both scale with the same exponent (0.50), confirming that parameters and data should grow at the same rate. The ratio D/N works out to roughly 20, meaning <strong>about 20 tokens per parameter</strong>.</p>

    <!-- Demo 6: Compute Calculator -->
    <div class="demo-container">
      <div class="demo-title">üßÆ Interactive ‚Äî Chinchilla Compute Calculator</div>
      <p style="font-family: system-ui, sans-serif; font-size: 15px; margin-bottom: 16px;">Enter your compute budget (in FLOPs) and see the Chinchilla-optimal allocation. Or enter a model size to see how much data and compute you need.</p>

      <div style="display:flex; gap:16px; margin-bottom:16px; flex-wrap:wrap;">
        <button class="btn" id="calcModeFlop" onclick="setCalcMode('flop')" style="flex:1;min-width:140px;">From Compute Budget</button>
        <button class="btn btn-outline" id="calcModeParam" onclick="setCalcMode('param')" style="flex:1;min-width:140px;">From Model Size</button>
      </div>

      <div id="calcFlop" class="input-group">
        <label style="font-size:14px;font-weight:600;">Compute: 10^</label>
        <input type="number" id="flopExp" value="23" min="18" max="28" step="0.1" oninput="calcFromFlops()">
        <span style="font-size:14px;color:var(--muted);">FLOPs</span>
      </div>

      <div id="calcParam" class="input-group" style="display:none;">
        <label style="font-size:14px;font-weight:600;">Parameters:</label>
        <input type="number" id="paramInput" value="70" min="0.1" max="10000" step="0.1" oninput="calcFromParams()">
        <span style="font-size:14px;color:var(--muted);">billion</span>
      </div>

      <div id="calcResults" style="display:grid; grid-template-columns:1fr 1fr 1fr; gap:12px; margin-top:16px;">
        <div class="hover-card" style="cursor:default;">
          <div class="hc-label">Optimal Parameters</div>
          <div class="hc-value" id="calcN">67B</div>
        </div>
        <div class="hover-card" style="cursor:default;">
          <div class="hc-label">Optimal Tokens</div>
          <div class="hc-value" id="calcD">1.4T</div>
        </div>
        <div class="hover-card" style="cursor:default;">
          <div class="hc-label">Total Compute</div>
          <div class="hc-value" id="calcC">10¬≤¬≥</div>
        </div>
      </div>

      <div class="demo-caption">Try plugging in famous models: GPT-3 had 175B params but only 300B tokens. According to Chinchilla, it should have had ~3.5T tokens!</div>
    </div>

    <p>Here's a fun exercise: plug in GPT-3's 175B parameters. According to the Chinchilla scaling law, it should have been trained on roughly <strong>3.5 trillion tokens</strong>. It was actually trained on 300 billion ‚Äî about <strong>12√ó too few</strong>. That's a lot of wasted potential.</p>
  </section>

  <!-- ============== SECTION VII ============== -->
  <section id="sec-kaplan" class="fade-in">
    <h2>VII. Kaplan vs. Hoffmann: Dueling Scaling Laws</h2>

    <p>The Chinchilla paper didn't just present new results ‚Äî it directly <strong>contradicted</strong> the most influential scaling paper in AI. The Kaplan et al. (2020) paper from OpenAI had argued for very different optimal scaling ratios.</p>

    <p>Kaplan found that <strong>parameters should scale ~3√ó faster than data</strong>. Double your compute? Make the model about 5√ó bigger but only train on 1.5√ó more tokens. This led to the "bigger model, same data" approach that dominated the field.</p>

    <p>Hoffmann et al. found the <strong>opposite</strong>: parameters and data should scale equally. So what went wrong with Kaplan's analysis?</p>

    <!-- Demo 7: Kaplan vs Chinchilla comparison -->
    <div class="demo-container">
      <div class="demo-title">‚öîÔ∏è Interactive ‚Äî Dueling Scaling Laws</div>
      <p style="font-family: system-ui, sans-serif; font-size: 15px; margin-bottom: 16px;">Use the slider to increase compute budget and see how the two scaling laws prescribe <em>very</em> different allocations.</p>

      <div class="slider-group">
        <label>Compute multiplier: <span id="duelMultLabel">1√ó</span> (relative to GPT-3's budget)</label>
        <input type="range" min="0" max="100" value="0" id="duelSlider" oninput="updateDuel()">
      </div>

      <div style="display:grid; grid-template-columns:1fr 1fr; gap:20px; margin-top:20px;">
        <div style="text-align:center;">
          <div style="font-family:system-ui,sans-serif; font-size:13px; font-weight:700; color:var(--muted); margin-bottom:8px;">Kaplan (2020) says:</div>
          <div style="font-family:system-ui,sans-serif; font-size:13px; margin-bottom:4px;">Parameters: <strong id="kaplanN">175B</strong></div>
          <div class="progress-bar-track" style="height:18px;">
            <div class="progress-bar-fill" id="kaplanNBar" style="width:35%; background:var(--orange); font-size:11px;"></div>
          </div>
          <div style="font-family:system-ui,sans-serif; font-size:13px; margin:8px 0 4px;">Tokens: <strong id="kaplanD">300B</strong></div>
          <div class="progress-bar-track" style="height:18px;">
            <div class="progress-bar-fill" id="kaplanDBar" style="width:6%; background:var(--orange); font-size:11px;"></div>
          </div>
        </div>
        <div style="text-align:center;">
          <div style="font-family:system-ui,sans-serif; font-size:13px; font-weight:700; color:var(--muted); margin-bottom:8px;">Chinchilla (2022) says:</div>
          <div style="font-family:system-ui,sans-serif; font-size:13px; margin-bottom:4px;">Parameters: <strong id="chinchN">175B</strong></div>
          <div class="progress-bar-track" style="height:18px;">
            <div class="progress-bar-fill" id="chinchNBar" style="width:35%; background:var(--accent); font-size:11px;"></div>
          </div>
          <div style="font-family:system-ui,sans-serif; font-size:13px; margin:8px 0 4px;">Tokens: <strong id="chinchD">3.5T</strong></div>
          <div class="progress-bar-track" style="height:18px;">
            <div class="progress-bar-fill" id="chinchDBar" style="width:70%; background:var(--accent); font-size:11px;"></div>
          </div>
        </div>
      </div>

      <div id="duelComment" class="result-display" style="margin-top:16px; font-size:14px;">
        At 1√ó GPT-3 compute, Kaplan would build GPT-3 (175B). Chinchilla would build a 67B model trained on 1.4T tokens.
      </div>

      <div class="demo-caption">As compute grows, the gap between the two prescriptions becomes enormous. Kaplan would build ever-larger undertrained giants.</div>
    </div>

    <p>The key methodological differences that led Kaplan astray:</p>

    <p><strong>1. Learning rate schedules.</strong> Kaplan's experiments used a fixed learning rate schedule that wasn't adjusted to match the number of training tokens. When training for longer, you need to decay the learning rate more gradually. Without this, longer training runs look artificially worse ‚Äî biasing the results toward bigger models with shorter training.</p>

    <p><strong>2. Not training to convergence.</strong> Many of Kaplan's smaller models may not have been trained long enough, making small models appear less capable than they actually are.</p>

    <p><strong>3. Narrow parameter range.</strong> Kaplan's experiments covered a narrower range of model sizes, making the extrapolation to very large scales less reliable.</p>

    <p>These sound like minor technical details, but they compounded into a conclusion that misdirected billions of dollars of compute investment. <strong>Methodological rigor matters</strong> ‚Äî especially when entire industries follow your prescriptions.</p>
  </section>

  <!-- ============== SECTION VIII ============== -->
  <section id="sec-tax" class="fade-in">
    <h2>VIII. The Chinchilla Tax</h2>

    <p>After the paper dropped, a delightful term entered the AI lexicon: the <strong>"Chinchilla tax."</strong> It refers to the extra inference cost you pay forever because you trained a model that's bigger than it needed to be.</p>

    <p>Here's the logic: Suppose you trained a 280B parameter model (like Gopher) when a compute-optimal 70B model (like Chinchilla) would have been just as good or better. Every single API call, every chat response, every inference step now costs you <strong>4√ó more</strong> than it should. Multiply that by millions of users and months of deployment, and you're talking about <strong>hundreds of millions of dollars</strong> in unnecessary inference costs.</p>

    <p>The training run is a one-time cost. Inference runs forever.</p>

    <!-- Demo 8: Chinchilla Tax Calculator -->
    <div class="demo-container">
      <div class="demo-title">üí∞ Interactive ‚Äî The Chinchilla Tax Calculator</div>
      <p style="font-family: system-ui, sans-serif; font-size: 15px; margin-bottom: 16px;">Estimate how much extra you'd spend serving an oversized model vs. the compute-optimal one.</p>

      <div class="slider-group">
        <label>Actual model size: <span id="taxActual">280B</span> parameters</label>
        <input type="range" min="10" max="600" value="280" id="taxActualSlider" oninput="updateTax()">
      </div>
      <div class="slider-group">
        <label>Chinchilla-optimal size: <span id="taxOptimal">70B</span> parameters</label>
        <input type="range" min="5" max="300" value="70" id="taxOptimalSlider" oninput="updateTax()">
      </div>
      <div class="slider-group">
        <label>Inference requests/day: <span id="taxRequests">10M</span></label>
        <input type="range" min="1" max="100" value="10" id="taxRequestSlider" oninput="updateTax()">
      </div>

      <div style="display:grid; grid-template-columns:1fr 1fr; gap:12px; margin-top:16px;">
        <div class="hover-card" style="cursor:default; border-color:var(--red);">
          <div class="hc-label">Oversized Model Cost</div>
          <div class="hc-value" id="taxCostBig" style="color:var(--red);">$840K</div>
          <div class="hc-label">per month</div>
        </div>
        <div class="hover-card" style="cursor:default; border-color:var(--green);">
          <div class="hc-label">Chinchilla-Optimal Cost</div>
          <div class="hc-value" id="taxCostSmall" style="color:var(--green);">$210K</div>
          <div class="hc-label">per month</div>
        </div>
      </div>

      <div class="result-display" id="taxSavings" style="font-size:18px; margin-top:12px;">
        üí∏ You'd waste $630K/month ‚Äî $7.6M/year ‚Äî serving the bigger model.
      </div>

      <div class="demo-caption">This is a simplified estimate, but the directional insight is clear: an oversized model is a gift that keeps on taking.</div>
    </div>

    <p>The "Chinchilla tax" is why this paper had such outsized impact on the industry. It wasn't just about training better models ‚Äî it was about <strong>the economics of deployment</strong>. In a world where LLMs are served to millions of users, inference costs dominate. A smaller model with the same (or better!) capabilities is worth enormously more than a bigger one.</p>

    <p>Some wags noted the irony: the entire industry had been voluntarily paying a massive Chinchilla tax for years ‚Äî because they were following scaling laws that told them bigger was always better.</p>
  </section>

  <!-- ============== SECTION IX ============== -->
  <section id="sec-industry" class="fade-in">
    <h2>IX. How This Changed Every AI Lab</h2>

    <p>The Chinchilla paper didn't just start an academic debate. It <strong>fundamentally changed how every major AI lab allocates compute</strong>. The evidence is visible in every major model released after March 2022.</p>

    <!-- Demo 9: Industry Impact Cards -->
    <div class="demo-container">
      <div class="demo-title">üè≠ Interactive ‚Äî The Post-Chinchilla Era</div>
      <p style="font-family: system-ui, sans-serif; font-size: 15px; margin-bottom: 16px;">Hover over each model to see how the Chinchilla paper influenced its design. Notice how the tokens-per-parameter ratio increased dramatically.</p>

      <div class="hover-grid" id="industryGrid">
        <div class="hover-card">
          <div class="hc-value" style="font-size:16px;">LLaMA</div>
          <div class="hc-label">Meta, Feb 2023</div>
          <div class="hc-value" style="font-size:14px;">65B / 1.4T</div>
          <div class="hc-label">~21 tok/param ‚úÖ</div>
          <div class="hc-detail">Explicitly cited Chinchilla. Trained the 7B model on 1T tokens ‚Äî far more than Kaplan would prescribe. Open-sourced, catalyzing the entire open-source LLM movement.</div>
        </div>
        <div class="hover-card">
          <div class="hc-value" style="font-size:16px;">PaLM 2</div>
          <div class="hc-label">Google, May 2023</div>
          <div class="hc-value" style="font-size:14px;">340B / 3.6T</div>
          <div class="hc-label">~10.6 tok/param</div>
          <div class="hc-detail">Google dramatically increased training data. PaLM 2 was smaller than PaLM 1 (540B) but significantly better ‚Äî a direct validation of Chinchilla's thesis.</div>
        </div>
        <div class="hover-card">
          <div class="hc-value" style="font-size:16px;">LLaMA 2</div>
          <div class="hc-label">Meta, Jul 2023</div>
          <div class="hc-value" style="font-size:14px;">70B / 2T</div>
          <div class="hc-label">~28 tok/param ‚úÖ</div>
          <div class="hc-detail">Meta went even beyond Chinchilla-optimal, training on more data than the scaling law would suggest. Turns out, even more data than 20√ó helps ‚Äî you can "over-train" for inference savings.</div>
        </div>
        <div class="hover-card">
          <div class="hc-value" style="font-size:16px;">Mistral 7B</div>
          <div class="hc-label">Mistral, Sep 2023</div>
          <div class="hc-value" style="font-size:14px;">7B / ?T</div>
          <div class="hc-label">Heavily data-fed</div>
          <div class="hc-detail">Mistral built a tiny 7B model that rivaled much larger ones. The Chinchilla philosophy taken to an extreme: maximum data, minimum parameters, maximum efficiency.</div>
        </div>
        <div class="hover-card">
          <div class="hc-value" style="font-size:16px;">LLaMA 3</div>
          <div class="hc-label">Meta, Apr 2024</div>
          <div class="hc-value" style="font-size:14px;">70B / 15T</div>
          <div class="hc-label">~214 tok/param üî•</div>
          <div class="hc-detail">Meta pushed far beyond Chinchilla-optimal, deliberately "over-training" the model so it would be smaller and cheaper at inference time. A strategy that only makes sense once you accept Chinchilla's framework.</div>
        </div>
        <div class="hover-card">
          <div class="hc-value" style="font-size:16px;">GPT-4</div>
          <div class="hc-label">OpenAI, Mar 2023</div>
          <div class="hc-value" style="font-size:14px;">~1.8T / ~13T</div>
          <div class="hc-label">MoE, ~7 tok/param</div>
          <div class="hc-detail">While details are sparse, leaks suggest GPT-4 is a Mixture-of-Experts model. The MoE architecture itself is partly a response to Chinchilla: get more parameters but only activate a fraction per token.</div>
        </div>
      </div>

      <div class="demo-caption">Notice the trend: post-Chinchilla models all dramatically increased their training data budgets. The parameter arms race was over.</div>
    </div>

    <p>The pattern is unmistakable. Before Chinchilla, labs scaled parameters 100√ó while barely increasing data 10√ó. After Chinchilla, the data budgets exploded: 1T, 2T, 15T tokens. Some teams deliberately <strong>"over-trained"</strong> their models on more data than Chinchilla-optimal, accepting slightly worse loss to get a smaller, cheaper model for deployment.</p>

    <p>This strategy ‚Äî training beyond the compute-optimal point for inference efficiency ‚Äî has become known as <strong>inference-optimal training</strong>, and it's a direct intellectual descendant of the Chinchilla paper. You can't think about inference optimization until you first accept the framework that says "bigger isn't always better."</p>
  </section>

  <!-- ============== SECTION X ============== -->
  <section id="sec-limits" class="fade-in">
    <h2>X. Limitations and the Data Wall</h2>

    <p>The Chinchilla paper is brilliant, but it's not without problems. The most immediate practical challenge is what some call the <strong>"data wall."</strong></p>

    <p>If a 1 trillion parameter model needs ~20 trillion tokens of high-quality training data, where exactly does all that data come from? The entire internet (as of 2022) was estimated to contain roughly <strong>5‚Äì10 trillion tokens</strong> of high-quality text. That's a hard ceiling.</p>

    <p>Several strategies have emerged to address this:</p>

    <!-- Demo 10: Data Wall Explorer -->
    <div class="demo-container">
      <div class="demo-title">üß± Interactive ‚Äî The Data Wall</div>
      <p style="font-family: system-ui, sans-serif; font-size: 15px; margin-bottom: 16px;">Select a model size to see how much data Chinchilla prescribes vs. how much quality data exists. Click each solution to learn more.</p>

      <div class="slider-group">
        <label>Model size: <span id="wallModelSize">500B</span> parameters</label>
        <input type="range" min="10" max="2000" value="500" id="wallSlider" oninput="updateWall()">
      </div>

      <div style="margin:16px 0;">
        <div style="font-family:system-ui,sans-serif;font-size:13px;font-weight:600;color:var(--muted);">Chinchilla-optimal data needed:</div>
        <div class="progress-bar-track" style="height:28px; margin-top:6px;">
          <div class="progress-bar-fill" id="wallNeedBar" style="width:50%; background: linear-gradient(90deg, var(--accent), var(--purple));">
            <span id="wallNeedLabel" style="font-size:12px;">10.0T tokens</span>
          </div>
        </div>
        <div style="font-family:system-ui,sans-serif;font-size:13px;font-weight:600;color:var(--muted);margin-top:12px;">Estimated high-quality internet text:</div>
        <div class="progress-bar-track" style="height:28px; margin-top:6px;">
          <div class="progress-bar-fill" style="width:35%; background: var(--green);">
            <span style="font-size:12px;">~7T tokens</span>
          </div>
        </div>
      </div>

      <div id="wallVerdict" class="result-display" style="font-size:14px;">
        ‚ö†Ô∏è A 500B model needs ~10T tokens. That's more high-quality text than exists on the public internet!
      </div>

      <div style="font-family:system-ui,sans-serif; font-size:14px; font-weight:700; margin:16px 0 8px;">Solutions being explored (click to expand):</div>

      <div class="quiz-options" id="dataWallSolutions">
        <div class="quiz-option" onclick="toggleSolution(this, 'Synthetic data generation uses existing LLMs to create new training data. LLaMA 3 reportedly used millions of synthetic examples. But there are concerns about \"model collapse\" ‚Äî training on AI-generated text can amplify biases and artifacts.')">ü§ñ Synthetic Data Generation</div>
        <div class="quiz-option" onclick="toggleSolution(this, 'Repeating high-quality data 2-4√ó has been shown to be less harmful than expected. Muennighoff et al. (2023) found that up to 4 epochs on quality data outperforms 1 epoch on lower-quality data.')">üîÑ Multi-Epoch Training</div>
        <div class="quiz-option" onclick="toggleSolution(this, 'Images, audio, video, and code contain enormous amounts of information. Training on multimodal data is one way to feed the data-hungry beast. A minute of video contains far more information than a page of text.')">üñºÔ∏è Multimodal Data</div>
        <div class="quiz-option" onclick="toggleSolution(this, 'Better data cleaning, deduplication, and curriculum design can extract more value from existing data. Quality over quantity ‚Äî sometimes cleaning your data is worth more than finding new data.')">üßπ Better Data Curation</div>
      </div>

      <div id="solutionDetail" style="font-family:system-ui,sans-serif; font-size:14px; color:var(--text); padding:12px 16px; background:var(--accent-light); border-radius:8px; margin-top:8px; display:none; line-height:1.5;"></div>

      <div class="demo-caption">The Chinchilla scaling law assumes infinite data. In practice, data is the bottleneck.</div>
    </div>

    <p>Other limitations worth noting:</p>

    <p><strong>The 20√ó ratio may shift with architecture changes.</strong> The Chinchilla experiments used standard dense Transformers. Mixture-of-Experts models, which activate only a fraction of parameters per token, have different compute profiles. The exact ratio might be different for different architectures.</p>

    <p><strong>Quality ‚â† Quantity.</strong> The paper treats all tokens as equal. But a token from a carefully curated textbook isn't the same as a token from a random Reddit thread. Data quality modifies the effective scaling law in ways that are still being researched.</p>

    <p><strong>Downstream performance vs. loss.</strong> Chinchilla optimizes pre-training loss (perplexity). But what matters in practice is downstream task performance after fine-tuning. The mapping from loss to usefulness isn't always straightforward.</p>
  </section>

  <!-- ============== SECTION XI ============== -->
  <section id="sec-legacy" class="fade-in">
    <h2>XI. Legacy and What Came Next</h2>

    <p>The Chinchilla paper is one of those rare works that instantly divides AI history into "before" and "after." Its influence extends far beyond the specific numbers it proposed.</p>

    <p><strong>It made data a first-class citizen.</strong> Before Chinchilla, the AI discourse was all about model size. After Chinchilla, labs started investing heavily in data pipelines, data quality, and data curation. Common Crawl refinement became as important as architecture innovation.</p>

    <p><strong>It enabled the open-source revolution.</strong> If bigger was always better, only the richest labs could compete. But Chinchilla showed that a well-trained 7B or 13B model could punch far above its weight. This opened the door for Meta's LLaMA, Mistral, and the entire open-source ecosystem.</p>

    <p><strong>It shifted the cost conversation from training to inference.</strong> Once you accept that a smaller model can match a larger one, the economics of deployment become paramount. This led to quantization, distillation, and inference-optimization becoming major research areas.</p>

    <!-- Demo 11: Quiz -->
    <div class="demo-container">
      <div class="demo-title">üß† Interactive ‚Äî Test Your Understanding</div>
      <p style="font-family: system-ui, sans-serif; font-size: 15px; margin-bottom: 16px;">Five questions to check if the Chinchilla insight has really sunk in.</p>

      <div id="quizContainer">
        <!-- Rendered by JS -->
      </div>
      <div id="quizScore" class="result-display" style="display:none; margin-top:16px;"></div>
    </div>

    <p>Perhaps the deepest legacy of the Chinchilla paper is <strong>philosophical</strong>. It taught the AI community that intuition about scaling can be dangerously wrong. The field's most prominent researchers, at the best-funded labs, had been following a suboptimal strategy for years ‚Äî because a single paper's methodology was slightly off, and nobody questioned it seriously until Hoffmann et al. did the rigorous experiments.</p>

    <p>If the smartest people in AI can be wrong about something this fundamental, what else might the field be getting wrong?</p>
  </section>

  <!-- ============== SECTION XII ============== -->
  <section id="sec-summary" class="fade-in">
    <h2>XII. Summary: The Scaling Cheat Sheet</h2>

    <p>Let's compress everything into a quick reference.</p>

    <!-- Demo 12: Interactive cheat sheet -->
    <div class="demo-container">
      <div class="demo-title">üìã Interactive ‚Äî Chinchilla Cheat Sheet</div>
      <p style="font-family: system-ui, sans-serif; font-size: 15px; margin-bottom: 16px;">Click on any row for a quick refresher on each key concept.</p>

      <table class="data-table" id="cheatTable">
        <thead>
          <tr>
            <th>Concept</th>
            <th>Pre-Chinchilla</th>
            <th>Post-Chinchilla</th>
          </tr>
        </thead>
        <tbody>
          <tr onclick="highlightRow(this)" style="cursor:pointer;" title="Click for detail">
            <td style="font-weight:600; text-align:left;">Scaling priority</td>
            <td>Parameters first</td>
            <td style="color:var(--green);font-weight:600;">Equal: params & data</td>
          </tr>
          <tr onclick="highlightRow(this)" style="cursor:pointer;" title="Click for detail">
            <td style="font-weight:600; text-align:left;">Tokens per parameter</td>
            <td>~1-5√ó</td>
            <td style="color:var(--green);font-weight:600;">~20√ó (or more)</td>
          </tr>
          <tr onclick="highlightRow(this)" style="cursor:pointer;" title="Click for detail">
            <td style="font-weight:600; text-align:left;">Key bottleneck</td>
            <td>Compute & model size</td>
            <td style="color:var(--green);font-weight:600;">Data quality & quantity</td>
          </tr>
          <tr onclick="highlightRow(this)" style="cursor:pointer;" title="Click for detail">
            <td style="font-weight:600; text-align:left;">Inference cost</td>
            <td>Ignored during training</td>
            <td style="color:var(--green);font-weight:600;">Central design concern</td>
          </tr>
          <tr onclick="highlightRow(this)" style="cursor:pointer;" title="Click for detail">
            <td style="font-weight:600; text-align:left;">Compute formula</td>
            <td>C ‚âà 6ND</td>
            <td style="color:var(--green);font-weight:600;">C ‚âà 6ND (same, better used)</td>
          </tr>
          <tr onclick="highlightRow(this)" style="cursor:pointer;" title="Click for detail">
            <td style="font-weight:600; text-align:left;">Optimal scaling exponent</td>
            <td>N ‚àù C‚Å∞¬∑‚Å∑¬≥ (Kaplan)</td>
            <td style="color:var(--green);font-weight:600;">N ‚àù C‚Å∞¬∑‚Åµ‚Å∞ (Hoffmann)</td>
          </tr>
        </tbody>
      </table>

      <div id="cheatDetail" style="font-family:system-ui,sans-serif; font-size:14px; padding:12px 16px; background:var(--callout-bg); border-radius:8px; margin-top:12px; display:none; line-height:1.5;"></div>

      <div class="demo-caption">This table might be the most expensive correction in AI history. Billions of dollars of compute were allocated based on the left column.</div>
    </div>

    <div class="callout">
      <div class="callout-title">The One-Sentence Summary</div>
      If you're training a language model, allocate your compute budget equally between model size and training data ‚Äî roughly <strong>20 tokens per parameter</strong> ‚Äî and you'll beat any model that puts its money primarily into more parameters.
    </div>

    <p>The Chinchilla paper is ultimately a story about <strong>humility in the face of data</strong>. The most expensive, prestigious models in the world were undertrained. The fix wasn't a better architecture or a clever trick ‚Äî it was simply reading more books. There's a lesson in that for all of us.</p>
  </section>

  <!-- ============== FURTHER RESOURCES ============== -->
  <section class="fade-in" style="margin-top:60px;">
    <h2>Further Resources</h2>
    <ul style="font-family:system-ui,sans-serif; font-size:15px; line-height:2;">
      <li><a href="https://arxiv.org/abs/2203.15556" target="_blank">Training Compute-Optimal Large Language Models (Hoffmann et al., 2022)</a> ‚Äî The original paper</li>
      <li><a href="https://arxiv.org/abs/2001.08361" target="_blank">Scaling Laws for Neural Language Models (Kaplan et al., 2020)</a> ‚Äî The earlier OpenAI scaling laws</li>
      <li><a href="https://arxiv.org/abs/2302.13971" target="_blank">LLaMA: Open and Efficient Foundation Language Models (Touvron et al., 2023)</a> ‚Äî Meta's Chinchilla-influenced open model</li>
      <li><a href="https://arxiv.org/abs/2305.16264" target="_blank">Scaling Data-Constrained Language Models (Muennighoff et al., 2023)</a> ‚Äî What happens when you run out of data</li>
      <li><a href="https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications" target="_blank">Chinchilla's Wild Implications (nostalgebraist)</a> ‚Äî Excellent community analysis</li>
    </ul>
  </section>
</div>

<!-- ============== FOOTER ============== -->
<footer>
  <p>Built with care. Inspired by <a href="https://explainers.blog" target="_blank">explainers.blog</a>.</p>
  <p style="margin-top: 8px;">An interactive explainer about Hoffmann et al., 2022.</p>
</footer>

<!-- ============== JAVASCRIPT ============== -->
<script>
// =============================================
// DEMO 1: Prediction Challenge
// =============================================
function updatePrediction() {
  const slider = document.getElementById('predSlider');
  const N = parseFloat(slider.value); // billions
  const C = 5.76e23;
  const D = C / (6 * N * 1e9) / 1e9; // billions of tokens

  document.getElementById('predParams').textContent = N + 'B';
  document.getElementById('predTokens').textContent = D >= 1000 ? (D/1000).toFixed(1) + 'T' : D.toFixed(0) + 'B';

  // Simplified loss model: L(N,D) = E + A/N^a + B/D^b
  const E = 1.69;
  const A = 406.4;
  const alpha = 0.34;
  const B = 410.7;
  const beta = 0.28;
  const loss = E + A / Math.pow(N * 1e9, alpha) + B / Math.pow(D * 1e9, beta);

  // Optimal loss (at ~67B params, ~1.4T tokens)
  const optN = 67e9, optD = 1.4e12;
  const optLoss = E + A / Math.pow(optN, alpha) + B / Math.pow(optD, beta);

  const pct = ((loss - optLoss) / optLoss * 100).toFixed(1);
  const el = document.getElementById('predResult');

  if (Math.abs(N - 67) < 20 && pct < 1) {
    el.innerHTML = `üéØ <strong>Estimated loss: ${loss.toFixed(3)}</strong> ‚Äî Excellent! You found the sweet spot! That's within ${pct}% of optimal.`;
    el.style.background = '#f0fdf4';
    el.style.color = '#16a34a';
  } else if (pct < 3) {
    el.innerHTML = `üìä Estimated loss: ${loss.toFixed(3)} ‚Äî Close! You're ${pct}% above optimal. Try shifting toward ~67B params.`;
    el.style.background = '#fefce8';
    el.style.color = '#a16207';
  } else {
    const hint = N > 67 ? 'Fewer parameters, more data!' : 'A bit more parameters would help.';
    el.innerHTML = `üìä Estimated loss: ${loss.toFixed(3)} ‚Äî That's ${pct}% above optimal. ${hint}`;
    el.style.background = '#fef2f2';
    el.style.color = '#dc2626';
  }
}

// =============================================
// DEMO 2: Timeline
// =============================================
function toggleTimeline(el) {
  el.classList.toggle('active');
}

// =============================================
// DEMO 3: Equal Scaling Visualizer
// =============================================
function updateEqualScale() {
  const exp = parseFloat(document.getElementById('computeSlider').value);
  const C = Math.pow(10, exp);

  // Chinchilla optimal scaling
  const Nopt = 0.0624 * Math.pow(C, 0.50);
  const Dopt = 0.2167 * Math.pow(C, 0.50);

  const maxN = 0.0624 * Math.pow(10, 13); // at 10^26
  const maxD = 0.2167 * Math.pow(10, 13);

  const nPct = Math.min(100, (Nopt / maxN) * 100);
  const dPct = Math.min(100, (Dopt / maxD) * 100);

  document.getElementById('paramBar').style.width = Math.max(5, nPct) + '%';
  document.getElementById('tokenBar').style.width = Math.max(5, dPct) + '%';

  document.getElementById('paramBarLabel').textContent = formatNum(Nopt);
  document.getElementById('tokenBarLabel').textContent = formatNum(Dopt);

  document.getElementById('computeLabel').textContent = '10^' + exp.toFixed(1);

  const ratio = Dopt / Nopt;
  document.getElementById('ratioDisplay').textContent = `Tokens-per-parameter ratio: ~${ratio.toFixed(1)}√ó`;
}

function formatNum(n) {
  if (n >= 1e12) return (n/1e12).toFixed(1) + 'T';
  if (n >= 1e9) return (n/1e9).toFixed(1) + 'B';
  if (n >= 1e6) return (n/1e6).toFixed(0) + 'M';
  if (n >= 1e3) return (n/1e3).toFixed(0) + 'K';
  return n.toFixed(0);
}

// =============================================
// DEMO 4: Three Approaches
// =============================================
let currentApproach = 0;
function showApproach(dir) {
  currentApproach = Math.max(0, Math.min(2, currentApproach + dir));
  document.querySelectorAll('.approach-panel').forEach((p, i) => {
    p.style.display = i === currentApproach ? 'block' : 'none';
  });
  document.querySelectorAll('#approachDots .step-dot').forEach((d, i) => {
    d.classList.toggle('active', i === currentApproach);
  });
}

// =============================================
// DEMO 5: Benchmark Comparison
// =============================================
const benchmarks = [
  { name: 'MMLU', gopher: 60.0, chinchilla: 67.5, detail: 'Massive Multitask Language Understanding ‚Äî 57 subjects from STEM to humanities. Chinchilla gained 7.5 points, a huge jump for this benchmark.' },
  { name: 'HellaSwag', gopher: 79.2, chinchilla: 80.8, detail: 'Commonsense reasoning about physical situations. Both models perform well, but Chinchilla edges ahead.' },
  { name: 'PIQA', gopher: 81.8, chinchilla: 81.8, detail: 'Physical Intuition QA ‚Äî a rare tie! Both models handle physical common sense equally well.' },
  { name: 'WinoGrande', gopher: 70.1, chinchilla: 74.9, detail: 'Winograd Schema Challenge ‚Äî pronoun resolution requiring world knowledge. Chinchilla shows a solid 4.8 point improvement.' },
  { name: 'ARC-e', gopher: 76.7, chinchilla: 80.5, detail: 'AI2 Reasoning Challenge (Easy). Even on "easy" reasoning tasks, Chinchilla gains 3.8 points.' },
  { name: 'ARC-c', gopher: 46.4, chinchilla: 48.6, detail: 'AI2 Reasoning Challenge (Challenge). Hard science reasoning. Every point here is hard-won.' },
  { name: 'BoolQ', gopher: 79.3, chinchilla: 83.7, detail: 'Boolean questions from natural language. Chinchilla\'s 4.4 point gain shows better language understanding.' },
  { name: 'TriviaQA', gopher: 57.2, chinchilla: 72.3, detail: 'Trivia question answering. A massive 15.1 point gain ‚Äî more data means more factual knowledge absorbed.' },
];

function renderBenchmarks() {
  const container = document.getElementById('benchmarkBars');
  let html = '<div style="display:grid; grid-template-columns: repeat(auto-fill, minmax(80px,1fr)); gap:6px;">';
  benchmarks.forEach((b, i) => {
    const winner = b.chinchilla >= b.gopher;
    html += `
      <div style="cursor:pointer; text-align:center;" onclick="showBenchmark(${i})">
        <div style="display:flex; gap:3px; justify-content:center; align-items:flex-end; height:120px;">
          <div style="width:28px; background:#e5e7eb; border-radius:4px 4px 0 0; height:${b.gopher * 1.1}px; transition:all 0.3s;" title="Gopher: ${b.gopher}"></div>
          <div style="width:28px; background:${winner ? '#16a34a' : '#2563eb'}; border-radius:4px 4px 0 0; height:${b.chinchilla * 1.1}px; transition:all 0.3s;" title="Chinchilla: ${b.chinchilla}"></div>
        </div>
        <div style="font-family:system-ui,sans-serif; font-size:10px; color:var(--muted); margin-top:4px;">${b.name}</div>
      </div>`;
  });
  html += '</div>';
  html += '<div style="display:flex;gap:16px;justify-content:center;margin-top:12px;font-family:system-ui,sans-serif;font-size:12px;">';
  html += '<span><span style="display:inline-block;width:12px;height:12px;background:#e5e7eb;border-radius:2px;vertical-align:middle;"></span> Gopher (280B)</span>';
  html += '<span><span style="display:inline-block;width:12px;height:12px;background:#16a34a;border-radius:2px;vertical-align:middle;"></span> Chinchilla (70B)</span>';
  html += '</div>';
  container.innerHTML = html;
}
function showBenchmark(i) {
  const b = benchmarks[i];
  const diff = (b.chinchilla - b.gopher).toFixed(1);
  const sign = diff >= 0 ? '+' : '';
  document.getElementById('benchmarkDetail').innerHTML = `<strong>${b.name}</strong>: Gopher ${b.gopher} ‚Üí Chinchilla ${b.chinchilla} (${sign}${diff}) ‚Äî ${b.detail}`;
}
renderBenchmarks();

// =============================================
// DEMO 6: Compute Calculator
// =============================================
let calcMode = 'flop';
function setCalcMode(mode) {
  calcMode = mode;
  document.getElementById('calcFlop').style.display = mode === 'flop' ? 'flex' : 'none';
  document.getElementById('calcParam').style.display = mode === 'param' ? 'flex' : 'none';
  document.getElementById('calcModeFlop').className = mode === 'flop' ? 'btn' : 'btn btn-outline';
  document.getElementById('calcModeParam').className = mode === 'param' ? 'btn' : 'btn btn-outline';
  if (mode === 'flop') calcFromFlops();
  else calcFromParams();
}
function calcFromFlops() {
  const exp = parseFloat(document.getElementById('flopExp').value) || 23;
  const C = Math.pow(10, exp);
  const Nopt = 0.0624 * Math.pow(C, 0.50);
  const Dopt = 0.2167 * Math.pow(C, 0.50);
  document.getElementById('calcN').textContent = formatNum(Nopt);
  document.getElementById('calcD').textContent = formatNum(Dopt);
  document.getElementById('calcC').textContent = '10^' + exp.toFixed(1);
}
function calcFromParams() {
  const Nb = parseFloat(document.getElementById('paramInput').value) || 70;
  const N = Nb * 1e9;
  const D = N * 20;
  const C = 6 * N * D;
  const cExp = Math.log10(C);
  document.getElementById('calcN').textContent = formatNum(N);
  document.getElementById('calcD').textContent = formatNum(D);
  document.getElementById('calcC').textContent = '10^' + cExp.toFixed(1);
}
calcFromFlops();

// =============================================
// DEMO 7: Kaplan vs Chinchilla
// =============================================
function updateDuel() {
  const v = parseFloat(document.getElementById('duelSlider').value);
  const mult = Math.pow(10, v / 25); // 1√ó to 10000√ó

  // Kaplan: N ‚àù C^0.73, D ‚àù C^0.27
  const kaplanN = 175 * Math.pow(mult, 0.73);
  const kaplanD = 300 * Math.pow(mult, 0.27);

  // Chinchilla: N ‚àù C^0.50, D ‚àù C^0.50
  const chinchN = 67 * Math.pow(mult, 0.50);
  const chinchD = 1400 * Math.pow(mult, 0.50);

  const maxN = 175 * Math.pow(10000, 0.73);
  const maxD = 1400 * Math.pow(10000, 0.50);

  document.getElementById('kaplanN').textContent = formatNum(kaplanN * 1e9);
  document.getElementById('kaplanD').textContent = formatNum(kaplanD * 1e9);
  document.getElementById('chinchN').textContent = formatNum(chinchN * 1e9);
  document.getElementById('chinchD').textContent = formatNum(chinchD * 1e9);

  document.getElementById('kaplanNBar').style.width = Math.max(3, kaplanN / maxN * 100) + '%';
  document.getElementById('kaplanDBar').style.width = Math.max(3, kaplanD / maxD * 100) + '%';
  document.getElementById('chinchNBar').style.width = Math.max(3, chinchN / maxN * 100) + '%';
  document.getElementById('chinchDBar').style.width = Math.max(3, chinchD / maxD * 100) + '%';

  document.getElementById('duelMultLabel').textContent = mult < 10 ? mult.toFixed(1) + '√ó' : Math.round(mult) + '√ó';

  const comment = document.getElementById('duelComment');
  if (mult < 1.5) {
    comment.textContent = `At ${mult.toFixed(1)}√ó GPT-3 compute, the two laws don't diverge much. But watch what happens at higher budgets...`;
  } else if (mult < 50) {
    comment.textContent = `At ${mult.toFixed(0)}√ó compute, Kaplan prescribes ${formatNum(kaplanN*1e9)} params (${formatNum(kaplanD*1e9)} tokens). Chinchilla says ${formatNum(chinchN*1e9)} params with ${formatNum(chinchD*1e9)} tokens ‚Äî a radically different strategy.`;
  } else {
    comment.textContent = `At ${Math.round(mult)}√ó compute, the gap is staggering. Kaplan wants a ${formatNum(kaplanN*1e9)}-param behemoth trained on just ${formatNum(kaplanD*1e9)} tokens. Chinchilla would build something much smaller but fed far more data.`;
  }
}
updateDuel();

// =============================================
// DEMO 8: Chinchilla Tax
// =============================================
function updateTax() {
  const actual = parseInt(document.getElementById('taxActualSlider').value);
  const optimal = parseInt(document.getElementById('taxOptimalSlider').value);
  const reqM = parseInt(document.getElementById('taxRequestSlider').value);

  document.getElementById('taxActual').textContent = actual + 'B';
  document.getElementById('taxOptimal').textContent = optimal + 'B';
  document.getElementById('taxRequests').textContent = reqM + 'M';

  // Simplified cost model: $0.001 per 1B params per 1K requests
  const costPerReqBig = actual * 0.000001; // dollars per request
  const costPerReqSmall = optimal * 0.000001;
  const monthlyBig = costPerReqBig * reqM * 1e6 * 30;
  const monthlySmall = costPerReqSmall * reqM * 1e6 * 30;

  document.getElementById('taxCostBig').textContent = '$' + formatMoney(monthlyBig);
  document.getElementById('taxCostSmall').textContent = '$' + formatMoney(monthlySmall);

  const savings = monthlyBig - monthlySmall;
  const yearly = savings * 12;
  document.getElementById('taxSavings').innerHTML = `üí∏ You'd waste <strong>${'$' + formatMoney(savings)}/month</strong> ‚Äî <strong>${'$' + formatMoney(yearly)}/year</strong> ‚Äî serving the bigger model.`;
}
function formatMoney(n) {
  if (n >= 1e9) return (n/1e9).toFixed(1) + 'B';
  if (n >= 1e6) return (n/1e6).toFixed(1) + 'M';
  if (n >= 1e3) return (n/1e3).toFixed(0) + 'K';
  return n.toFixed(0);
}
updateTax();

// =============================================
// DEMO 10: Data Wall
// =============================================
function updateWall() {
  const N = parseInt(document.getElementById('wallSlider').value);
  document.getElementById('wallModelSize').textContent = N + 'B';
  const tokensNeeded = N * 20; // billions
  const maxTokens = 40000; // 40T as max bar

  const pct = Math.min(100, tokensNeeded / maxTokens * 100);
  const bar = document.getElementById('wallNeedBar');
  bar.style.width = Math.max(5, pct) + '%';
  document.getElementById('wallNeedLabel').textContent = formatNum(tokensNeeded * 1e9) + ' tokens';

  const internetTokens = 7000; // 7T
  const verdict = document.getElementById('wallVerdict');
  if (tokensNeeded <= internetTokens) {
    verdict.innerHTML = `‚úÖ A ${N}B model needs ~${formatNum(tokensNeeded * 1e9)} tokens. That's feasible with current high-quality data (~7T tokens available).`;
    verdict.style.color = '#16a34a';
  } else {
    const ratio = (tokensNeeded / internetTokens).toFixed(1);
    verdict.innerHTML = `‚ö†Ô∏è A ${N}B model needs ~${formatNum(tokensNeeded * 1e9)} tokens. That's ${ratio}√ó more than estimated high-quality internet text!`;
    verdict.style.color = '#dc2626';
  }
}
updateWall();

function toggleSolution(el, detail) {
  const detailDiv = document.getElementById('solutionDetail');
  const allOpts = document.querySelectorAll('#dataWallSolutions .quiz-option');
  allOpts.forEach(o => o.style.borderColor = 'var(--demo-border)');
  el.style.borderColor = 'var(--accent)';
  detailDiv.textContent = detail;
  detailDiv.style.display = 'block';
}

// =============================================
// DEMO 11: Quiz
// =============================================
const quizQuestions = [
  {
    q: "According to the Chinchilla paper, if you double your compute budget, what should you do?",
    opts: [
      "Double the model size, keep data the same",
      "Increase both model size and data by ~1.4√ó each",
      "Keep model size the same, double the data",
      "Triple the model size, halve the data"
    ],
    correct: 1,
    explanation: "Since C ‚àù N √ó D and both N and D should scale as C^0.5, doubling C means multiplying each by ‚àö2 ‚âà 1.41."
  },
  {
    q: "Approximately how many tokens per parameter does Chinchilla prescribe?",
    opts: ["~1-2 tokens", "~5 tokens", "~20 tokens", "~100 tokens"],
    correct: 2,
    explanation: "The Chinchilla-optimal ratio is approximately 20 tokens per parameter. This was 10-20√ó more data than most existing models used."
  },
  {
    q: "Why did Kaplan's earlier scaling laws lead labs astray?",
    opts: [
      "Kaplan used the wrong programming language",
      "Kaplan's learning rate schedules weren't adjusted for longer training runs",
      "Kaplan only studied small models under 1M parameters",
      "Kaplan's paper was never peer-reviewed"
    ],
    correct: 1,
    explanation: "Kaplan's experiments used fixed learning rate schedules that didn't properly account for longer training durations, making longer training (more data) look artificially worse."
  },
  {
    q: "What is the 'Chinchilla tax'?",
    opts: [
      "A fee DeepMind charges for using their results",
      "The extra inference cost of serving an over-parameterized model forever",
      "The cost of collecting enough training data",
      "The regulatory burden of training large AI models"
    ],
    correct: 1,
    explanation: "The Chinchilla tax refers to the ongoing inference cost penalty of serving a model that's larger than it needed to be ‚Äî because every API call costs more with more parameters."
  },
  {
    q: "How did Chinchilla compare to Gopher despite being 4√ó smaller?",
    opts: [
      "It was slightly worse on most benchmarks",
      "It performed identically",
      "It outperformed Gopher on virtually all benchmarks",
      "It was better only on language tasks, worse on reasoning"
    ],
    correct: 2,
    explanation: "Chinchilla uniformly outperformed Gopher across nearly all evaluated benchmarks, despite having only 70B parameters vs. Gopher's 280B ‚Äî using the same total compute."
  }
];

function renderQuiz() {
  const container = document.getElementById('quizContainer');
  let html = '';
  quizQuestions.forEach((q, qi) => {
    html += `<div id="quiz-q${qi}" style="margin-bottom:24px;">
      <p style="font-family:system-ui,sans-serif;font-size:15px;font-weight:600;margin-bottom:10px;">${qi+1}. ${q.q}</p>
      <div class="quiz-options">`;
    q.opts.forEach((opt, oi) => {
      html += `<div class="quiz-option" onclick="answerQuiz(${qi},${oi})" id="q${qi}o${oi}">${opt}</div>`;
    });
    html += `</div>
    <div class="quiz-feedback" id="q${qi}fb"></div>
    </div>`;
  });
  container.innerHTML = html;
}
let quizAnswered = new Set();
let quizCorrect = 0;
function answerQuiz(qi, oi) {
  if (quizAnswered.has(qi)) return;
  quizAnswered.add(qi);
  const q = quizQuestions[qi];
  const correct = oi === q.correct;
  if (correct) quizCorrect++;

  document.getElementById(`q${qi}o${oi}`).className = 'quiz-option ' + (correct ? 'correct' : 'wrong');
  if (!correct) {
    document.getElementById(`q${qi}o${q.correct}`).className = 'quiz-option correct';
  }

  const fb = document.getElementById(`q${qi}fb`);
  fb.style.display = 'block';
  fb.style.background = correct ? '#f0fdf4' : '#fef2f2';
  fb.style.color = correct ? '#16a34a' : '#dc2626';
  fb.innerHTML = (correct ? '‚úÖ Correct! ' : '‚ùå Not quite. ') + q.explanation;

  if (quizAnswered.size === quizQuestions.length) {
    const score = document.getElementById('quizScore');
    score.style.display = 'block';
    score.innerHTML = `You scored <strong>${quizCorrect}/${quizQuestions.length}</strong>. ${quizCorrect === 5 ? 'üéâ Perfect! You\'ve internalized the Chinchilla insight.' : quizCorrect >= 3 ? 'üëç Solid understanding!' : 'üìñ Worth re-reading some sections above!'}`;
  }
}
renderQuiz();

// =============================================
// DEMO 12: Cheat Sheet
// =============================================
const cheatDetails = [
  "Pre-Chinchilla, labs prioritized parameter count (make the model bigger!). Chinchilla showed that parameters and training data should scale in lockstep ‚Äî equal priority to both.",
  "GPT-3 had ~1.7 tokens per parameter. Chinchilla showed the optimal is ~20 tokens per parameter ‚Äî an order of magnitude more data-hungry than the industry assumed.",
  "The bottleneck shifted from 'how big can we make the model?' to 'where do we find enough high-quality training data?' This spawned a data gold rush.",
  "Pre-Chinchilla, inference cost was an afterthought. Post-Chinchilla, everyone realized smaller well-trained models save enormously at deployment time. Inference-optimal training became a strategy.",
  "The FLOPs formula C ‚âà 6ND didn't change ‚Äî it's a property of transformer architecture. What changed was how labs choose N and D given a fixed C.",
  "Kaplan found N ‚àù C^0.73 (parameters dominate). Hoffmann found N ‚àù C^0.50 (equal scaling). The difference between these exponents redirected billions of dollars of compute."
];
function highlightRow(row) {
  const rows = document.querySelectorAll('#cheatTable tbody tr');
  let idx = -1;
  rows.forEach((r, i) => {
    r.style.background = '';
    if (r === row) idx = i;
  });
  row.style.background = 'var(--accent-light)';
  const detail = document.getElementById('cheatDetail');
  detail.style.display = 'block';
  detail.textContent = cheatDetails[idx] || '';
}

// =============================================
// Scroll animations
// =============================================
const observer = new IntersectionObserver((entries) => {
  entries.forEach(entry => {
    if (entry.isIntersecting) {
      entry.target.classList.add('visible');
    }
  });
}, { threshold: 0.1, rootMargin: '0px 0px -40px 0px' });

document.querySelectorAll('.fade-in').forEach(el => observer.observe(el));

// =============================================
// Hero animation
// =============================================
(function animateHero() {
  const circle = document.getElementById('chinchCircle');
  if (!circle) return;
  let t = 0;
  function tick() {
    t += 0.03;
    const scale = 1 + Math.sin(t) * 0.03;
    circle.setAttribute('r', 38 * scale);
    requestAnimationFrame(tick);
  }
  tick();
})();
</script>
</body>
</html>
